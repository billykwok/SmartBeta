{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cE1kSExBZtrv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def to_month_id(year, month):\n",
    "  return (year - 1996) * 12 + month - 6\n",
    "\n",
    "n_features = 12 # or 54\n",
    "lookback = 3\n",
    "chosen_stocks = [\"AOL\"] # \"AMZN\", \"MSFT\", \"IBM\", \"INTC\", \"QCOM\", \"NVDA\", \"IBM\", \"ADBE\", \"EBAY\", \"CSCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1525763876411,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "6vUqDR0MYQP1",
    "outputId": "ddc259d0-e2b6-4b12-e849-c602f12d1a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "dfFeature = pd.read_csv(\"./lstm_2004_12.csv\")\n",
    "# dfFeature.loc[dfFeature[\"return\"] == 0, \"return\"] = 1\n",
    "dfFeature = dfFeature[dfFeature.QAId.isin(chosen_stocks)]\n",
    "features = dfFeature.drop(columns=['month_id', 'QAId']).as_matrix()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1382,
     "status": "ok",
     "timestamp": 1525763878070,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "DwavdGT5wkIz",
    "outputId": "37dc165a-26f5-4bf4-c283-d635448bd707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizedFeatures = MinMaxScaler().fit_transform(features) \\\n",
    "                                   .reshape(157, len(chosen_stocks), n_features) \\\n",
    "                                   .reshape(157, len(chosen_stocks) * n_features)\n",
    "print(normalizedFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2118,
     "status": "ok",
     "timestamp": 1525763881161,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "ECiEkdPQ410b",
    "outputId": "7cada92e-95ac-400d-987d-cd9c6101549c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "dfTarget = pd.read_csv(\"./return_2004_40.csv\")\n",
    "dfTarget[\"return\"] = np.sign(dfTarget[\"return\"])\n",
    "# dfTarget.loc[dfTarget[\"return\"] == 0, \"return\"] = 1\n",
    "dfTarget = dfTarget[dfTarget.QAId.isin(chosen_stocks)]\n",
    "dfTarget = dfTarget[dfTarget.month_id >= (to_month_id(2004, 1) + lookback)]\n",
    "targets = MinMaxScaler().fit_transform(dfTarget.drop(columns=['month_id', 'QAId']).as_matrix())\n",
    "y = targets.reshape(157 - lookback + 1, len(chosen_stocks))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1525763882966,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "Vo9QHxamwmbL",
    "outputId": "9ef70b36-2a3c-44be-a6ed-22930b63500a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billykwok/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "splittedFeature = normalizedFeatures\n",
    "print(splittedFeature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2pHmaWhfZ4gm"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "x = np.zeros((157 - lookback + 1, lookback, n_features * len(chosen_stocks)))\n",
    "y_mock = np.zeros((157, len(chosen_stocks)))\n",
    "\n",
    "i = 0\n",
    "for train, test in TimeseriesGenerator(splittedFeature, y_mock, length=lookback, batch_size=1):\n",
    "  if i > 157 - lookback:\n",
    "    break\n",
    "  x[i] = train[0]\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1525763885182,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "SkuyoR1bZ699",
    "outputId": "d82f1d27-ee9e-4198-f364-67e97179fdaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 3, 12)\n",
      "(81, 1)\n",
      "(36, 3, 12)\n",
      "(36, 1)\n",
      "(26, 3, 12)\n",
      "(26, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "test_months = 26\n",
    "end_point = 157 - lookback + 1\n",
    "split_point = 157 - lookback + 1 - test_months\n",
    "\n",
    "x_train = x[0:split_point - 48].reshape(split_point - 48, lookback, n_features * len(chosen_stocks))\n",
    "y_train = y[0:split_point - 48].reshape(split_point - 48, len(chosen_stocks))\n",
    "x_validate = x[split_point - 36:split_point].reshape(36, lookback, n_features * len(chosen_stocks))\n",
    "y_validate = y[split_point - 36:split_point].reshape(36, len(chosen_stocks))\n",
    "x_test = x[split_point:end_point].reshape(test_months, lookback, n_features * len(chosen_stocks))\n",
    "y_test = y[split_point:end_point].reshape(test_months, len(chosen_stocks))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validate.shape)\n",
    "print(y_validate.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# ps = PredefinedSplit(np.append(np.negative(np.ones(60 * 465)), np.zeros(24 * 465))).split(x_train)\n",
    "\n",
    "# for train_ids, test_ids in ps:\n",
    "#   print(str(train_ids) + \", \" + str(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Co9Gsz7aZ_hb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Flatten, CuDNNLSTM\n",
    "from keras.regularizers import l1_l2, l2\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "\n",
    "np.random.seed(4103)\n",
    "\n",
    "def create_model(*param):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=256, input_shape=(lookback, n_features * len(chosen_stocks)), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=64, return_sequences=False))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(len(chosen_stocks), activation=\"relu\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, decay=0.0), metrics=['accuracy'], *param)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13823,
     "status": "error",
     "timestamp": 1525764743477,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "0l18zd8xaQqR",
    "outputId": "47960b89-37a1-460d-aa9c-a58bbdff370f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81 samples, validate on 36 samples\n",
      "Epoch 1/200\n",
      "81/81 [==============================] - 3s 42ms/step - loss: 8.5565 - acc: 0.4691 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 2/200\n",
      "81/81 [==============================] - 0s 760us/step - loss: 8.5565 - acc: 0.4691 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 3/200\n",
      "81/81 [==============================] - 0s 793us/step - loss: 8.4531 - acc: 0.4691 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 4/200\n",
      "81/81 [==============================] - 0s 744us/step - loss: 8.3193 - acc: 0.4691 - val_loss: 8.9296 - val_acc: 0.3333\n",
      "Epoch 5/200\n",
      "81/81 [==============================] - 0s 722us/step - loss: 5.2865 - acc: 0.4691 - val_loss: 3.7403 - val_acc: 0.3333\n",
      "Epoch 6/200\n",
      "81/81 [==============================] - 0s 707us/step - loss: 2.8097 - acc: 0.4691 - val_loss: 3.2362 - val_acc: 0.3333\n",
      "Epoch 7/200\n",
      "81/81 [==============================] - 0s 693us/step - loss: 2.4853 - acc: 0.4691 - val_loss: 2.9771 - val_acc: 0.3333\n",
      "Epoch 8/200\n",
      "81/81 [==============================] - 0s 726us/step - loss: 2.3186 - acc: 0.4691 - val_loss: 2.8097 - val_acc: 0.3333\n",
      "Epoch 9/200\n",
      "81/81 [==============================] - 0s 704us/step - loss: 2.2142 - acc: 0.4691 - val_loss: 2.6891 - val_acc: 0.3333\n",
      "Epoch 10/200\n",
      "81/81 [==============================] - 0s 717us/step - loss: 2.1190 - acc: 0.4691 - val_loss: 2.5957 - val_acc: 0.3333\n",
      "Epoch 11/200\n",
      "81/81 [==============================] - 0s 709us/step - loss: 2.0528 - acc: 0.4691 - val_loss: 2.5206 - val_acc: 0.3333\n",
      "Epoch 12/200\n",
      "81/81 [==============================] - 0s 718us/step - loss: 1.9940 - acc: 0.4691 - val_loss: 2.4567 - val_acc: 0.3333\n",
      "Epoch 13/200\n",
      "81/81 [==============================] - 0s 782us/step - loss: 1.9618 - acc: 0.4691 - val_loss: 2.4008 - val_acc: 0.3333\n",
      "Epoch 14/200\n",
      "81/81 [==============================] - 0s 828us/step - loss: 1.9137 - acc: 0.4691 - val_loss: 2.3510 - val_acc: 0.3333\n",
      "Epoch 15/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 1.8844 - acc: 0.4691 - val_loss: 2.3072 - val_acc: 0.3333\n",
      "Epoch 16/200\n",
      "81/81 [==============================] - 0s 814us/step - loss: 1.8450 - acc: 0.4691 - val_loss: 2.2670 - val_acc: 0.3333\n",
      "Epoch 17/200\n",
      "81/81 [==============================] - 0s 891us/step - loss: 1.8150 - acc: 0.4691 - val_loss: 2.2303 - val_acc: 0.3333\n",
      "Epoch 18/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 1.7970 - acc: 0.4691 - val_loss: 2.1960 - val_acc: 0.3333\n",
      "Epoch 19/200\n",
      "81/81 [==============================] - 0s 825us/step - loss: 1.7670 - acc: 0.4691 - val_loss: 2.1640 - val_acc: 0.3333\n",
      "Epoch 20/200\n",
      "81/81 [==============================] - 0s 820us/step - loss: 1.7621 - acc: 0.4691 - val_loss: 2.1338 - val_acc: 0.3333\n",
      "Epoch 21/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 1.7164 - acc: 0.4691 - val_loss: 2.1047 - val_acc: 0.3333\n",
      "Epoch 22/200\n",
      "81/81 [==============================] - 0s 786us/step - loss: 1.6861 - acc: 0.4691 - val_loss: 2.0766 - val_acc: 0.3333\n",
      "Epoch 23/200\n",
      "81/81 [==============================] - 0s 863us/step - loss: 1.6694 - acc: 0.4691 - val_loss: 2.0491 - val_acc: 0.3333\n",
      "Epoch 24/200\n",
      "81/81 [==============================] - 0s 836us/step - loss: 1.6561 - acc: 0.4691 - val_loss: 2.0224 - val_acc: 0.3333\n",
      "Epoch 25/200\n",
      "81/81 [==============================] - 0s 824us/step - loss: 1.6377 - acc: 0.4691 - val_loss: 1.9961 - val_acc: 0.3333\n",
      "Epoch 26/200\n",
      "81/81 [==============================] - 0s 767us/step - loss: 1.6177 - acc: 0.4691 - val_loss: 1.9705 - val_acc: 0.3333\n",
      "Epoch 27/200\n",
      "81/81 [==============================] - 0s 768us/step - loss: 1.5905 - acc: 0.4691 - val_loss: 1.9453 - val_acc: 0.3333\n",
      "Epoch 28/200\n",
      "81/81 [==============================] - 0s 903us/step - loss: 1.5742 - acc: 0.4691 - val_loss: 1.9206 - val_acc: 0.3333\n",
      "Epoch 29/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 1.5658 - acc: 0.4691 - val_loss: 1.8962 - val_acc: 0.3333\n",
      "Epoch 30/200\n",
      "81/81 [==============================] - 0s 857us/step - loss: 1.5322 - acc: 0.4691 - val_loss: 1.8723 - val_acc: 0.3333\n",
      "Epoch 31/200\n",
      "81/81 [==============================] - 0s 800us/step - loss: 1.5146 - acc: 0.4691 - val_loss: 1.8491 - val_acc: 0.3333\n",
      "Epoch 32/200\n",
      "81/81 [==============================] - 0s 814us/step - loss: 1.5034 - acc: 0.4691 - val_loss: 1.8260 - val_acc: 0.3333\n",
      "Epoch 33/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 1.4721 - acc: 0.4691 - val_loss: 1.8032 - val_acc: 0.3333\n",
      "Epoch 34/200\n",
      "81/81 [==============================] - 0s 821us/step - loss: 1.4618 - acc: 0.4691 - val_loss: 1.7811 - val_acc: 0.3333\n",
      "Epoch 35/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 1.4443 - acc: 0.4691 - val_loss: 1.7594 - val_acc: 0.3333\n",
      "Epoch 36/200\n",
      "81/81 [==============================] - 0s 790us/step - loss: 1.4309 - acc: 0.4691 - val_loss: 1.7373 - val_acc: 0.3333\n",
      "Epoch 37/200\n",
      "81/81 [==============================] - 0s 865us/step - loss: 1.4064 - acc: 0.4691 - val_loss: 1.7154 - val_acc: 0.3333\n",
      "Epoch 38/200\n",
      "81/81 [==============================] - 0s 824us/step - loss: 1.3959 - acc: 0.4691 - val_loss: 1.6937 - val_acc: 0.3333\n",
      "Epoch 39/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 1.3857 - acc: 0.4691 - val_loss: 1.6719 - val_acc: 0.3333\n",
      "Epoch 40/200\n",
      "81/81 [==============================] - 0s 767us/step - loss: 1.3690 - acc: 0.4691 - val_loss: 1.6505 - val_acc: 0.3333\n",
      "Epoch 41/200\n",
      "81/81 [==============================] - 0s 765us/step - loss: 1.3451 - acc: 0.4691 - val_loss: 1.6292 - val_acc: 0.3333\n",
      "Epoch 42/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 1.3320 - acc: 0.4691 - val_loss: 1.6075 - val_acc: 0.3333\n",
      "Epoch 43/200\n",
      "81/81 [==============================] - 0s 761us/step - loss: 1.3043 - acc: 0.4691 - val_loss: 1.5861 - val_acc: 0.3333\n",
      "Epoch 44/200\n",
      "81/81 [==============================] - 0s 768us/step - loss: 1.3048 - acc: 0.4691 - val_loss: 1.5650 - val_acc: 0.3333\n",
      "Epoch 45/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 1.2700 - acc: 0.4691 - val_loss: 1.5437 - val_acc: 0.3333\n",
      "Epoch 46/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 1.2603 - acc: 0.4691 - val_loss: 1.5224 - val_acc: 0.3333\n",
      "Epoch 47/200\n",
      "81/81 [==============================] - 0s 818us/step - loss: 1.2356 - acc: 0.4691 - val_loss: 1.5013 - val_acc: 0.3333\n",
      "Epoch 48/200\n",
      "81/81 [==============================] - 0s 870us/step - loss: 1.2393 - acc: 0.4691 - val_loss: 1.4805 - val_acc: 0.3333\n",
      "Epoch 49/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.2192 - acc: 0.4691 - val_loss: 1.4596 - val_acc: 0.3333\n",
      "Epoch 50/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.2156 - acc: 0.4691 - val_loss: 1.4388 - val_acc: 0.3333\n",
      "Epoch 51/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.1976 - acc: 0.4691 - val_loss: 1.4185 - val_acc: 0.3333\n",
      "Epoch 52/200\n",
      "81/81 [==============================] - 0s 941us/step - loss: 1.1542 - acc: 0.4691 - val_loss: 1.3978 - val_acc: 0.3333\n",
      "Epoch 53/200\n",
      "81/81 [==============================] - 0s 794us/step - loss: 1.1603 - acc: 0.4691 - val_loss: 1.3776 - val_acc: 0.3333\n",
      "Epoch 54/200\n",
      "81/81 [==============================] - 0s 819us/step - loss: 1.1192 - acc: 0.4691 - val_loss: 1.3575 - val_acc: 0.3333\n",
      "Epoch 55/200\n",
      "81/81 [==============================] - 0s 771us/step - loss: 1.1231 - acc: 0.4691 - val_loss: 1.3370 - val_acc: 0.3333\n",
      "Epoch 56/200\n",
      "81/81 [==============================] - 0s 793us/step - loss: 1.1028 - acc: 0.4691 - val_loss: 1.3168 - val_acc: 0.3333\n",
      "Epoch 57/200\n",
      "81/81 [==============================] - 0s 783us/step - loss: 1.0923 - acc: 0.4691 - val_loss: 1.2964 - val_acc: 0.3333\n",
      "Epoch 58/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.0664 - acc: 0.4691 - val_loss: 1.2764 - val_acc: 0.3333\n",
      "Epoch 59/200\n",
      "81/81 [==============================] - 0s 847us/step - loss: 1.0605 - acc: 0.4691 - val_loss: 1.2567 - val_acc: 0.3333\n",
      "Epoch 60/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 1.0565 - acc: 0.4691 - val_loss: 1.2371 - val_acc: 0.3333\n",
      "Epoch 61/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.0299 - acc: 0.4691 - val_loss: 1.2173 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.0158 - acc: 0.4691 - val_loss: 1.1973 - val_acc: 0.3333\n",
      "Epoch 63/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 1.0050 - acc: 0.4691 - val_loss: 1.1778 - val_acc: 0.3333\n",
      "Epoch 64/200\n",
      "81/81 [==============================] - 0s 747us/step - loss: 0.9974 - acc: 0.4691 - val_loss: 1.1586 - val_acc: 0.3333\n",
      "Epoch 65/200\n",
      "81/81 [==============================] - 0s 760us/step - loss: 0.9616 - acc: 0.4691 - val_loss: 1.1395 - val_acc: 0.3333\n",
      "Epoch 66/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.9583 - acc: 0.4691 - val_loss: 1.1202 - val_acc: 0.3333\n",
      "Epoch 67/200\n",
      "81/81 [==============================] - 0s 785us/step - loss: 0.9578 - acc: 0.4691 - val_loss: 1.1013 - val_acc: 0.3333\n",
      "Epoch 68/200\n",
      "81/81 [==============================] - 0s 794us/step - loss: 0.9269 - acc: 0.4691 - val_loss: 1.0822 - val_acc: 0.3333\n",
      "Epoch 69/200\n",
      "81/81 [==============================] - 0s 775us/step - loss: 0.9156 - acc: 0.4691 - val_loss: 1.0639 - val_acc: 0.3333\n",
      "Epoch 70/200\n",
      "81/81 [==============================] - 0s 784us/step - loss: 0.9184 - acc: 0.4691 - val_loss: 1.0453 - val_acc: 0.3333\n",
      "Epoch 71/200\n",
      "81/81 [==============================] - 0s 767us/step - loss: 0.8819 - acc: 0.4691 - val_loss: 1.0273 - val_acc: 0.3333\n",
      "Epoch 72/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.8825 - acc: 0.4691 - val_loss: 1.0094 - val_acc: 0.3333\n",
      "Epoch 73/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 0.8622 - acc: 0.4691 - val_loss: 0.9916 - val_acc: 0.3333\n",
      "Epoch 74/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.8597 - acc: 0.4691 - val_loss: 0.9740 - val_acc: 0.3333\n",
      "Epoch 75/200\n",
      "81/81 [==============================] - 0s 777us/step - loss: 0.8470 - acc: 0.4691 - val_loss: 0.9559 - val_acc: 0.3333\n",
      "Epoch 76/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.8234 - acc: 0.4691 - val_loss: 0.9394 - val_acc: 0.3333\n",
      "Epoch 77/200\n",
      "81/81 [==============================] - 0s 800us/step - loss: 0.8175 - acc: 0.4691 - val_loss: 0.9228 - val_acc: 0.3333\n",
      "Epoch 78/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.8122 - acc: 0.4691 - val_loss: 0.9063 - val_acc: 0.3333\n",
      "Epoch 79/200\n",
      "81/81 [==============================] - 0s 798us/step - loss: 0.7977 - acc: 0.4691 - val_loss: 0.8905 - val_acc: 0.3333\n",
      "Epoch 80/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 0.7910 - acc: 0.4691 - val_loss: 0.8748 - val_acc: 0.3333\n",
      "Epoch 81/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 0.7795 - acc: 0.4691 - val_loss: 0.8596 - val_acc: 0.3333\n",
      "Epoch 82/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 0.7680 - acc: 0.4691 - val_loss: 0.8456 - val_acc: 0.3333\n",
      "Epoch 83/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.7696 - acc: 0.4691 - val_loss: 0.8315 - val_acc: 0.3333\n",
      "Epoch 84/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 0.7651 - acc: 0.4691 - val_loss: 0.8179 - val_acc: 0.3333\n",
      "Epoch 85/200\n",
      "81/81 [==============================] - 0s 816us/step - loss: 0.7575 - acc: 0.4568 - val_loss: 0.8045 - val_acc: 0.3333\n",
      "Epoch 86/200\n",
      "81/81 [==============================] - 0s 820us/step - loss: 0.7474 - acc: 0.4691 - val_loss: 0.7919 - val_acc: 0.3333\n",
      "Epoch 87/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.7426 - acc: 0.4691 - val_loss: 0.7798 - val_acc: 0.3333\n",
      "Epoch 88/200\n",
      "81/81 [==============================] - 0s 809us/step - loss: 0.7393 - acc: 0.4444 - val_loss: 0.7680 - val_acc: 0.3333\n",
      "Epoch 89/200\n",
      "81/81 [==============================] - 0s 792us/step - loss: 0.7179 - acc: 0.4938 - val_loss: 0.7571 - val_acc: 0.3333\n",
      "Epoch 90/200\n",
      "81/81 [==============================] - 0s 844us/step - loss: 0.7191 - acc: 0.5062 - val_loss: 0.7476 - val_acc: 0.3333\n",
      "Epoch 91/200\n",
      "81/81 [==============================] - 0s 795us/step - loss: 0.6938 - acc: 0.4691 - val_loss: 0.7382 - val_acc: 0.3333\n",
      "Epoch 92/200\n",
      "81/81 [==============================] - 0s 908us/step - loss: 0.7205 - acc: 0.4938 - val_loss: 0.7297 - val_acc: 0.3611\n",
      "Epoch 93/200\n",
      "81/81 [==============================] - 0s 849us/step - loss: 0.7187 - acc: 0.5185 - val_loss: 0.7219 - val_acc: 0.3889\n",
      "Epoch 94/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.6952 - acc: 0.5679 - val_loss: 0.7149 - val_acc: 0.4722\n",
      "Epoch 95/200\n",
      "81/81 [==============================] - 0s 811us/step - loss: 0.6913 - acc: 0.5802 - val_loss: 0.7076 - val_acc: 0.4722\n",
      "Epoch 96/200\n",
      "81/81 [==============================] - 0s 790us/step - loss: 0.7046 - acc: 0.5432 - val_loss: 0.7015 - val_acc: 0.5000\n",
      "Epoch 97/200\n",
      "81/81 [==============================] - 0s 812us/step - loss: 0.7114 - acc: 0.4938 - val_loss: 0.6958 - val_acc: 0.5278\n",
      "Epoch 98/200\n",
      "81/81 [==============================] - 0s 845us/step - loss: 0.7030 - acc: 0.5185 - val_loss: 0.6914 - val_acc: 0.5833\n",
      "Epoch 99/200\n",
      "81/81 [==============================] - 0s 820us/step - loss: 0.7000 - acc: 0.5309 - val_loss: 0.6874 - val_acc: 0.5833\n",
      "Epoch 100/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.7031 - acc: 0.4815 - val_loss: 0.6843 - val_acc: 0.5556\n",
      "Epoch 101/200\n",
      "81/81 [==============================] - 0s 918us/step - loss: 0.7192 - acc: 0.4198 - val_loss: 0.6817 - val_acc: 0.5556\n",
      "Epoch 102/200\n",
      "81/81 [==============================] - 0s 881us/step - loss: 0.7012 - acc: 0.4938 - val_loss: 0.6799 - val_acc: 0.5556\n",
      "Epoch 103/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.6980 - acc: 0.5432 - val_loss: 0.6781 - val_acc: 0.5556\n",
      "Epoch 104/200\n",
      "81/81 [==============================] - 0s 806us/step - loss: 0.6972 - acc: 0.5309 - val_loss: 0.6762 - val_acc: 0.5278\n",
      "Epoch 105/200\n",
      "81/81 [==============================] - 0s 798us/step - loss: 0.6894 - acc: 0.5802 - val_loss: 0.6752 - val_acc: 0.5278\n",
      "Epoch 106/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.6846 - acc: 0.6296 - val_loss: 0.6737 - val_acc: 0.5278\n",
      "Epoch 107/200\n",
      "81/81 [==============================] - 0s 831us/step - loss: 0.6794 - acc: 0.5926 - val_loss: 0.6725 - val_acc: 0.5556\n",
      "Epoch 108/200\n",
      "81/81 [==============================] - 0s 842us/step - loss: 0.6854 - acc: 0.5679 - val_loss: 0.6717 - val_acc: 0.5278\n",
      "Epoch 109/200\n",
      "81/81 [==============================] - 0s 855us/step - loss: 0.7079 - acc: 0.5556 - val_loss: 0.6710 - val_acc: 0.5278\n",
      "Epoch 110/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 0.6852 - acc: 0.5432 - val_loss: 0.6701 - val_acc: 0.5000\n",
      "Epoch 111/200\n",
      "81/81 [==============================] - 0s 851us/step - loss: 0.6978 - acc: 0.5556 - val_loss: 0.6696 - val_acc: 0.5278\n",
      "Epoch 112/200\n",
      "81/81 [==============================] - 0s 901us/step - loss: 0.6940 - acc: 0.5309 - val_loss: 0.6694 - val_acc: 0.5556\n",
      "Epoch 113/200\n",
      "81/81 [==============================] - 0s 980us/step - loss: 0.6636 - acc: 0.6049 - val_loss: 0.6696 - val_acc: 0.5556\n",
      "Epoch 114/200\n",
      "81/81 [==============================] - 0s 891us/step - loss: 0.6727 - acc: 0.5802 - val_loss: 0.6700 - val_acc: 0.5278\n",
      "Epoch 115/200\n",
      "81/81 [==============================] - 0s 837us/step - loss: 0.6916 - acc: 0.5432 - val_loss: 0.6700 - val_acc: 0.5278\n",
      "Epoch 116/200\n",
      "81/81 [==============================] - 0s 843us/step - loss: 0.6830 - acc: 0.5926 - val_loss: 0.6698 - val_acc: 0.5556\n",
      "Epoch 117/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.6823 - acc: 0.5802 - val_loss: 0.6693 - val_acc: 0.5556\n",
      "Epoch 118/200\n",
      "81/81 [==============================] - 0s 794us/step - loss: 0.6767 - acc: 0.5926 - val_loss: 0.6694 - val_acc: 0.5556\n",
      "Epoch 119/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.6897 - acc: 0.5556 - val_loss: 0.6688 - val_acc: 0.5556\n",
      "Epoch 120/200\n",
      "81/81 [==============================] - 0s 864us/step - loss: 0.6962 - acc: 0.5309 - val_loss: 0.6681 - val_acc: 0.5556\n",
      "Epoch 121/200\n",
      "81/81 [==============================] - 0s 843us/step - loss: 0.6877 - acc: 0.5679 - val_loss: 0.6671 - val_acc: 0.5556\n",
      "Epoch 122/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.6953 - acc: 0.5679 - val_loss: 0.6669 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "81/81 [==============================] - 0s 834us/step - loss: 0.6886 - acc: 0.5185 - val_loss: 0.6665 - val_acc: 0.5833\n",
      "Epoch 124/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.7000 - acc: 0.5432 - val_loss: 0.6664 - val_acc: 0.5833\n",
      "Epoch 125/200\n",
      "81/81 [==============================] - 0s 784us/step - loss: 0.7010 - acc: 0.5062 - val_loss: 0.6663 - val_acc: 0.5833\n",
      "Epoch 126/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.6776 - acc: 0.6296 - val_loss: 0.6655 - val_acc: 0.6111\n",
      "Epoch 127/200\n",
      "81/81 [==============================] - 0s 780us/step - loss: 0.6705 - acc: 0.5309 - val_loss: 0.6648 - val_acc: 0.6111\n",
      "Epoch 128/200\n",
      "81/81 [==============================] - 0s 758us/step - loss: 0.6817 - acc: 0.5185 - val_loss: 0.6648 - val_acc: 0.6111\n",
      "Epoch 129/200\n",
      "81/81 [==============================] - 0s 810us/step - loss: 0.6643 - acc: 0.6543 - val_loss: 0.6645 - val_acc: 0.6389\n",
      "Epoch 130/200\n",
      "81/81 [==============================] - 0s 777us/step - loss: 0.6858 - acc: 0.5062 - val_loss: 0.6651 - val_acc: 0.6111\n",
      "Epoch 131/200\n",
      "81/81 [==============================] - 0s 778us/step - loss: 0.6720 - acc: 0.5926 - val_loss: 0.6652 - val_acc: 0.6111\n",
      "Epoch 132/200\n",
      "81/81 [==============================] - 0s 762us/step - loss: 0.6755 - acc: 0.6296 - val_loss: 0.6650 - val_acc: 0.6389\n",
      "Epoch 133/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.6752 - acc: 0.5679 - val_loss: 0.6655 - val_acc: 0.6111\n",
      "Epoch 134/200\n",
      "81/81 [==============================] - 0s 835us/step - loss: 0.6831 - acc: 0.5679 - val_loss: 0.6655 - val_acc: 0.6111\n",
      "Epoch 135/200\n",
      "81/81 [==============================] - 0s 778us/step - loss: 0.6849 - acc: 0.5679 - val_loss: 0.6659 - val_acc: 0.6111\n",
      "Epoch 136/200\n",
      "81/81 [==============================] - 0s 824us/step - loss: 0.6928 - acc: 0.5926 - val_loss: 0.6658 - val_acc: 0.6111\n",
      "Epoch 137/200\n",
      "81/81 [==============================] - 0s 812us/step - loss: 0.6702 - acc: 0.6049 - val_loss: 0.6655 - val_acc: 0.6111\n",
      "Epoch 138/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.6745 - acc: 0.5679 - val_loss: 0.6658 - val_acc: 0.6111\n",
      "Epoch 139/200\n",
      "81/81 [==============================] - 0s 830us/step - loss: 0.6509 - acc: 0.6296 - val_loss: 0.6653 - val_acc: 0.6389\n",
      "Epoch 140/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.6912 - acc: 0.5802 - val_loss: 0.6651 - val_acc: 0.6389\n",
      "Epoch 141/200\n",
      "81/81 [==============================] - 0s 829us/step - loss: 0.6724 - acc: 0.5926 - val_loss: 0.6651 - val_acc: 0.6389\n",
      "Epoch 142/200\n",
      "81/81 [==============================] - 0s 811us/step - loss: 0.6782 - acc: 0.6173 - val_loss: 0.6649 - val_acc: 0.6667\n",
      "Epoch 143/200\n",
      "81/81 [==============================] - 0s 821us/step - loss: 0.6854 - acc: 0.5062 - val_loss: 0.6649 - val_acc: 0.6667\n",
      "Epoch 144/200\n",
      "81/81 [==============================] - 0s 818us/step - loss: 0.6669 - acc: 0.5802 - val_loss: 0.6647 - val_acc: 0.6667\n",
      "Epoch 145/200\n",
      "81/81 [==============================] - 0s 851us/step - loss: 0.7029 - acc: 0.5185 - val_loss: 0.6646 - val_acc: 0.6667\n",
      "Epoch 146/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.6793 - acc: 0.5802 - val_loss: 0.6648 - val_acc: 0.6667\n",
      "Epoch 147/200\n",
      "81/81 [==============================] - 0s 872us/step - loss: 0.6876 - acc: 0.5802 - val_loss: 0.6663 - val_acc: 0.6111\n",
      "Epoch 148/200\n",
      "81/81 [==============================] - 0s 864us/step - loss: 0.6739 - acc: 0.6543 - val_loss: 0.6677 - val_acc: 0.6111\n",
      "Epoch 149/200\n",
      "81/81 [==============================] - 0s 826us/step - loss: 0.6961 - acc: 0.5556 - val_loss: 0.6695 - val_acc: 0.5833\n",
      "Epoch 150/200\n",
      "81/81 [==============================] - 0s 877us/step - loss: 0.6728 - acc: 0.6173 - val_loss: 0.6705 - val_acc: 0.5833\n",
      "Epoch 151/200\n",
      "81/81 [==============================] - 0s 878us/step - loss: 0.6562 - acc: 0.6543 - val_loss: 0.6714 - val_acc: 0.5556\n",
      "Epoch 152/200\n",
      "81/81 [==============================] - 0s 868us/step - loss: 0.6597 - acc: 0.6049 - val_loss: 0.6719 - val_acc: 0.5278\n",
      "Epoch 153/200\n",
      "81/81 [==============================] - 0s 883us/step - loss: 0.6835 - acc: 0.5432 - val_loss: 0.6728 - val_acc: 0.5278\n",
      "Epoch 154/200\n",
      "81/81 [==============================] - 0s 868us/step - loss: 0.6677 - acc: 0.5556 - val_loss: 0.6723 - val_acc: 0.5278\n",
      "Epoch 155/200\n",
      "81/81 [==============================] - 0s 890us/step - loss: 0.6633 - acc: 0.6173 - val_loss: 0.6717 - val_acc: 0.5278\n",
      "Epoch 156/200\n",
      "81/81 [==============================] - 0s 905us/step - loss: 0.6801 - acc: 0.5679 - val_loss: 0.6710 - val_acc: 0.5556\n",
      "Epoch 157/200\n",
      "81/81 [==============================] - 0s 907us/step - loss: 0.6834 - acc: 0.5309 - val_loss: 0.6701 - val_acc: 0.6111\n",
      "Epoch 158/200\n",
      "81/81 [==============================] - 0s 913us/step - loss: 0.6922 - acc: 0.5802 - val_loss: 0.6695 - val_acc: 0.6111\n",
      "Epoch 159/200\n",
      "81/81 [==============================] - 0s 911us/step - loss: 0.6735 - acc: 0.5802 - val_loss: 0.6691 - val_acc: 0.6111\n",
      "Epoch 160/200\n",
      "81/81 [==============================] - 0s 944us/step - loss: 0.6753 - acc: 0.6049 - val_loss: 0.6686 - val_acc: 0.6111\n",
      "Epoch 161/200\n",
      "81/81 [==============================] - 0s 859us/step - loss: 0.6847 - acc: 0.5926 - val_loss: 0.6682 - val_acc: 0.6111\n",
      "Epoch 162/200\n",
      "81/81 [==============================] - 0s 899us/step - loss: 0.6793 - acc: 0.5802 - val_loss: 0.6681 - val_acc: 0.6111\n",
      "Epoch 163/200\n",
      "81/81 [==============================] - 0s 933us/step - loss: 0.6710 - acc: 0.6049 - val_loss: 0.6680 - val_acc: 0.6111\n",
      "Epoch 164/200\n",
      "81/81 [==============================] - 0s 902us/step - loss: 0.6826 - acc: 0.5926 - val_loss: 0.6691 - val_acc: 0.6111\n",
      "Epoch 165/200\n",
      "81/81 [==============================] - 0s 882us/step - loss: 0.6823 - acc: 0.5679 - val_loss: 0.6704 - val_acc: 0.6111\n",
      "Epoch 166/200\n",
      "81/81 [==============================] - 0s 882us/step - loss: 0.6716 - acc: 0.6420 - val_loss: 0.6710 - val_acc: 0.5833\n",
      "Epoch 167/200\n",
      "81/81 [==============================] - 0s 909us/step - loss: 0.7043 - acc: 0.5556 - val_loss: 0.6721 - val_acc: 0.5278\n",
      "Epoch 168/200\n",
      "81/81 [==============================] - 0s 864us/step - loss: 0.6571 - acc: 0.6296 - val_loss: 0.6726 - val_acc: 0.5556\n",
      "Epoch 169/200\n",
      "81/81 [==============================] - 0s 896us/step - loss: 0.6880 - acc: 0.5185 - val_loss: 0.6741 - val_acc: 0.5278\n",
      "Epoch 170/200\n",
      "81/81 [==============================] - 0s 889us/step - loss: 0.6829 - acc: 0.5309 - val_loss: 0.6743 - val_acc: 0.5278\n",
      "Epoch 171/200\n",
      "81/81 [==============================] - 0s 873us/step - loss: 0.6878 - acc: 0.6049 - val_loss: 0.6745 - val_acc: 0.5278\n",
      "Epoch 172/200\n",
      "81/81 [==============================] - 0s 870us/step - loss: 0.6652 - acc: 0.5926 - val_loss: 0.6751 - val_acc: 0.5278\n",
      "Epoch 173/200\n",
      "81/81 [==============================] - 0s 908us/step - loss: 0.6615 - acc: 0.6049 - val_loss: 0.6754 - val_acc: 0.5278\n",
      "Epoch 174/200\n",
      "81/81 [==============================] - 0s 904us/step - loss: 0.6729 - acc: 0.5556 - val_loss: 0.6757 - val_acc: 0.5278\n",
      "Epoch 175/200\n",
      "81/81 [==============================] - 0s 870us/step - loss: 0.6687 - acc: 0.6173 - val_loss: 0.6760 - val_acc: 0.5556\n",
      "Epoch 176/200\n",
      "81/81 [==============================] - 0s 836us/step - loss: 0.6843 - acc: 0.5309 - val_loss: 0.6757 - val_acc: 0.5556\n",
      "Epoch 177/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.6735 - acc: 0.6049 - val_loss: 0.6763 - val_acc: 0.5556\n",
      "Epoch 178/200\n",
      "81/81 [==============================] - 0s 835us/step - loss: 0.6871 - acc: 0.5309 - val_loss: 0.6759 - val_acc: 0.5556\n",
      "Epoch 179/200\n",
      "81/81 [==============================] - 0s 825us/step - loss: 0.6766 - acc: 0.6049 - val_loss: 0.6773 - val_acc: 0.5556\n",
      "Epoch 180/200\n",
      "81/81 [==============================] - 0s 810us/step - loss: 0.6691 - acc: 0.5556 - val_loss: 0.6770 - val_acc: 0.5556\n",
      "Epoch 181/200\n",
      "81/81 [==============================] - 0s 852us/step - loss: 0.6859 - acc: 0.6049 - val_loss: 0.6768 - val_acc: 0.5556\n",
      "Epoch 182/200\n",
      "81/81 [==============================] - 0s 834us/step - loss: 0.6771 - acc: 0.6049 - val_loss: 0.6779 - val_acc: 0.5556\n",
      "Epoch 183/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.6596 - acc: 0.6173 - val_loss: 0.6782 - val_acc: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "81/81 [==============================] - 0s 866us/step - loss: 0.6609 - acc: 0.6790 - val_loss: 0.6769 - val_acc: 0.5833\n",
      "Epoch 185/200\n",
      "81/81 [==============================] - 0s 797us/step - loss: 0.6593 - acc: 0.6296 - val_loss: 0.6759 - val_acc: 0.5833\n",
      "Epoch 186/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 0.6720 - acc: 0.5802 - val_loss: 0.6749 - val_acc: 0.5833\n",
      "Epoch 187/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.6446 - acc: 0.6543 - val_loss: 0.6747 - val_acc: 0.5833\n",
      "Epoch 188/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.6693 - acc: 0.5802 - val_loss: 0.6734 - val_acc: 0.5278\n",
      "Epoch 189/200\n",
      "81/81 [==============================] - 0s 792us/step - loss: 0.6598 - acc: 0.6296 - val_loss: 0.6730 - val_acc: 0.5278\n",
      "Epoch 190/200\n",
      "81/81 [==============================] - 0s 809us/step - loss: 0.6805 - acc: 0.5679 - val_loss: 0.6729 - val_acc: 0.5278\n",
      "Epoch 191/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 0.6772 - acc: 0.6173 - val_loss: 0.6732 - val_acc: 0.5278\n",
      "Epoch 192/200\n",
      "81/81 [==============================] - 0s 811us/step - loss: 0.6677 - acc: 0.5926 - val_loss: 0.6737 - val_acc: 0.5278\n",
      "Epoch 193/200\n",
      "81/81 [==============================] - 0s 823us/step - loss: 0.6833 - acc: 0.5802 - val_loss: 0.6740 - val_acc: 0.5556\n",
      "Epoch 194/200\n",
      "81/81 [==============================] - 0s 828us/step - loss: 0.6729 - acc: 0.6049 - val_loss: 0.6741 - val_acc: 0.5556\n",
      "Epoch 195/200\n",
      "81/81 [==============================] - 0s 843us/step - loss: 0.6751 - acc: 0.6049 - val_loss: 0.6745 - val_acc: 0.5556\n",
      "Epoch 196/200\n",
      "81/81 [==============================] - 0s 836us/step - loss: 0.6705 - acc: 0.5926 - val_loss: 0.6747 - val_acc: 0.5556\n",
      "Epoch 197/200\n",
      "81/81 [==============================] - 0s 774us/step - loss: 0.6751 - acc: 0.6049 - val_loss: 0.6747 - val_acc: 0.5556\n",
      "Epoch 198/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.6667 - acc: 0.5556 - val_loss: 0.6757 - val_acc: 0.5556\n",
      "Epoch 199/200\n",
      "81/81 [==============================] - 0s 859us/step - loss: 0.6729 - acc: 0.6173 - val_loss: 0.6761 - val_acc: 0.5556\n",
      "Epoch 200/200\n",
      "81/81 [==============================] - 0s 852us/step - loss: 0.6781 - acc: 0.6049 - val_loss: 0.6774 - val_acc: 0.5833\n",
      "<keras.callbacks.History object at 0x1a3c5c1278>\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "\n",
    "model = create_model()\n",
    "train_score = model.fit(x_train, y_train, validation_data=(x_validate, y_validate), batch_size=batch_size, epochs=200)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1157,
     "status": "error",
     "timestamp": 1525763095237,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "yycG9fQgaTWR",
    "outputId": "6bc04992-aa67-445c-a5dc-4034a301660a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 280us/step\n",
      "loss: 0.7195595502853394\n",
      "acc: 0.3461538553237915\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a3c5c1e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "for i, mn in enumerate(model.metrics_names):\n",
    "  print(mn + \": \" + str(test_score[i]))\n",
    "\n",
    "plt.plot(train_score.history[\"loss\"])\n",
    "plt.plot(train_score.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fYwRfk9VfdKR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "def threshold_tester(result, target, a, b):\n",
    "  for t in np.arange(a, b + 0.05, 0.05):\n",
    "    print(\"thresholder: \" + str(t))\n",
    "    print(classification_report(target, (result >= t).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2921
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "error",
     "timestamp": 1525762991314,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "zQ-ZbuKz3MVJ",
    "outputId": "12722ed0-2978-4350-8022-880def161a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.519091  ]\n",
      " [0.4878207 ]\n",
      " [0.44051448]\n",
      " [0.45606306]\n",
      " [0.43784532]\n",
      " [0.4625986 ]\n",
      " [0.4965909 ]\n",
      " [0.47987875]\n",
      " [0.43690395]\n",
      " [0.40582985]\n",
      " [0.42096534]\n",
      " [0.4727516 ]\n",
      " [0.4305416 ]\n",
      " [0.42704627]\n",
      " [0.44702792]\n",
      " [0.41906995]\n",
      " [0.45179015]\n",
      " [0.448288  ]\n",
      " [0.46446756]\n",
      " [0.48156378]\n",
      " [0.46924078]\n",
      " [0.43604392]\n",
      " [0.46608135]\n",
      " [0.5224509 ]\n",
      " [0.49373478]\n",
      " [0.500103  ]]\n",
      "thresholder: 0.1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.15000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.20000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.25000000000000006\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.30000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.3500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.40000000000000013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00         8\n",
      "        1.0       0.69      1.00      0.82        18\n",
      "\n",
      "avg / total       0.48      0.69      0.57        26\n",
      "\n",
      "thresholder: 0.45000000000000007\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.36      0.50      0.42         8\n",
      "        1.0       0.73      0.61      0.67        18\n",
      "\n",
      "avg / total       0.62      0.58      0.59        26\n",
      "\n",
      "thresholder: 0.5000000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.30      0.88      0.45         8\n",
      "        1.0       0.67      0.11      0.19        18\n",
      "\n",
      "avg / total       0.56      0.35      0.27        26\n",
      "\n",
      "thresholder: 0.5500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.6000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.6500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.7000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.7500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.8000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.8500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n",
      "thresholder: 0.9000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.31      1.00      0.47         8\n",
      "        1.0       0.00      0.00      0.00        18\n",
      "\n",
      "avg / total       0.09      0.31      0.14        26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billykwok/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(x_test, batch_size=batch_size)\n",
    "print(result)\n",
    "threshold_tester(result, y_test, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1525762727684,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "qcA2GSqdINA-",
    "outputId": "b09b9962-ff00-4a8f-c628-e0e0f9877b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOL\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t1\tGain\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "0.0\t1\tLoss\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "1.0\t1\tGain\n",
      "1.0\t1\tGain\n",
      "0.0\t1\tLoss\n",
      "\n",
      "\n",
      "[{'month_id': 223, 'QAId': 'AOL'}, {'month_id': 229, 'QAId': 'AOL'}, {'month_id': 246, 'QAId': 'AOL'}, {'month_id': 247, 'QAId': 'AOL'}, {'month_id': 248, 'QAId': 'AOL'}]\n",
      "[{'month_id': 225, 'QAId': 'AOL'}, {'month_id': 227, 'QAId': 'AOL'}, {'month_id': 231, 'QAId': 'AOL'}, {'month_id': 232, 'QAId': 'AOL'}, {'month_id': 233, 'QAId': 'AOL'}, {'month_id': 235, 'QAId': 'AOL'}, {'month_id': 236, 'QAId': 'AOL'}, {'month_id': 238, 'QAId': 'AOL'}, {'month_id': 244, 'QAId': 'AOL'}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "buy_list = []\n",
    "sell_list = []\n",
    "\n",
    "for j, stock in enumerate(chosen_stocks):\n",
    "  print(stock)\n",
    "  sorted_result = sorted(map(lambda x: x[j], result))\n",
    "  midpt = (sorted_result[-2] + sorted_result[1]) / 2\n",
    "  upper_threshold = midpt * 1.05\n",
    "  lower_threshold = midpt * 0.95\n",
    "  \n",
    "  print(\"Target\\tPredict\\tConsequence\")\n",
    "  for i, r in enumerate(result):\n",
    "    prediction = r[j].item()\n",
    "    target = y_test[i][j].item()\n",
    "    buy_or_sell = 1 if prediction > upper_threshold else (-1 if prediction < lower_threshold else 0)\n",
    "    if prediction > upper_threshold:\n",
    "      buy_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    if prediction < lower_threshold:\n",
    "      sell_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    \n",
    "    to_print = str(target) + \"\\t\" + str(buy_or_sell)\n",
    "    if (buy_or_sell == -1 and target == 0) or (buy_or_sell == 1 and target == 1):\n",
    "      print(to_print + \"\\tGain\")\n",
    "    elif (buy_or_sell == -1 and target == 1) or (buy_or_sell == 1 and target == 0):\n",
    "      print(to_print + \"\\tLoss\")\n",
    "    else:\n",
    "      print(to_print + \"\\tNothing\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "print(buy_list)\n",
    "print(sell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1525762773847,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "tTp8yfv_ZSQv",
    "outputId": "0ee9cf79-1947-460b-d0cd-95a36f3d0f9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>QAId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>229</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id QAId\n",
       "0       223  AOL\n",
       "1       229  AOL\n",
       "2       246  AOL\n",
       "3       247  AOL\n",
       "4       248  AOL"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBuy = pd.DataFrame(buy_list, columns=[\"month_id\", \"QAId\"])\n",
    "dfSell = pd.DataFrame(sell_list, columns=[\"month_id\", \"QAId\"])\n",
    "dfBuy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>QAId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>227</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>233</td>\n",
       "      <td>AOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id QAId\n",
       "0       225  AOL\n",
       "1       227  AOL\n",
       "2       231  AOL\n",
       "3       232  AOL\n",
       "4       233  AOL"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSell.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t2lSM7yu3dY7"
   },
   "outputs": [],
   "source": [
    "filename_base = \"_\".join([stock.lower() for stock in chosen_stocks])\n",
    "# filename_model = \"./\" + filename_base + \"_model.h5\"\n",
    "# filename_weights = \"./\" + filename_base + \"_weights.h5\"\n",
    "filename_output_buy = \"./\" + filename_base + \"_output_buy.h5\"\n",
    "filename_output_sell = \"./\" + filename_base + \"_output_sell.h5\"\n",
    "\n",
    "# model.save(filename_model)\n",
    "# model.save_weights(filename_weights)\n",
    "dfBuy.to_csv(filename_output_buy, index=False)\n",
    "dfSell.to_csv(filename_output_sell, index=False)\n",
    "\n",
    "# files.download(filename_model)\n",
    "# files.download(filename_weights)\n",
    "# files.download(filename_output_buy)\n",
    "# files.download(filename_output_sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "er00PkDpHjZ_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MSFT",
   "provenance": [
    {
     "file_id": "1DNgXa_HOyZehXtWnJ_rmfuch2xQ-gRCg",
     "timestamp": 1525762850051
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
