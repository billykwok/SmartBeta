{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cE1kSExBZtrv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def to_month_id(year, month):\n",
    "  return (year - 1996) * 12 + month - 6\n",
    "\n",
    "n_features = 12 # or 54\n",
    "lookback = 12\n",
    "chosen_stocks = [\n",
    "    \"AAPL\",\n",
    "    \"ADM\",\n",
    "    \"AFL\",\n",
    "    \"AIG\",\n",
    "    \"ALL\",\n",
    "    \"AMZN\",\n",
    "    \"AXP\",\n",
    "    \"COF\",\n",
    "    \"COST\",\n",
    "    \"CVS\",\n",
    "    \"DIS\",\n",
    "    \"F\",\n",
    "    \"FDX\",\n",
    "    \"HAL\",\n",
    "    \"HIG\",\n",
    "    \"LOW\",\n",
    "    \"MET\",\n",
    "    \"PGR\",\n",
    "    \"PRU\",\n",
    "    \"SBUX\",\n",
    "    \"SPC\",\n",
    "    \"WAG\"\n",
    "] # \"AMZN\", \"MSFT\", \"IBM\", \"INTC\", \"QCOM\", \"NVDA\", \"IBM\", \"ADBE\", \"EBAY\", \"CSCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1525763876411,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "6vUqDR0MYQP1",
    "outputId": "ddc259d0-e2b6-4b12-e849-c602f12d1a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3454, 12)\n"
     ]
    }
   ],
   "source": [
    "dfFeature = pd.read_csv(\"./lstm_2004_12.csv\")\n",
    "# dfFeature.loc[dfFeature[\"return\"] == 0, \"return\"] = 1\n",
    "dfFeature = dfFeature[dfFeature.QAId.isin(chosen_stocks)]\n",
    "features = dfFeature.drop(columns=['month_id', 'QAId']).as_matrix()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1382,
     "status": "ok",
     "timestamp": 1525763878070,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "DwavdGT5wkIz",
    "outputId": "37dc165a-26f5-4bf4-c283-d635448bd707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 264)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizedFeatures = MinMaxScaler().fit_transform(features) \\\n",
    "                                   .reshape(157, len(chosen_stocks), n_features) \\\n",
    "                                   .reshape(157, len(chosen_stocks) * n_features)\n",
    "print(normalizedFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2118,
     "status": "ok",
     "timestamp": 1525763881161,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "ECiEkdPQ410b",
    "outputId": "7cada92e-95ac-400d-987d-cd9c6101549c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 22)\n"
     ]
    }
   ],
   "source": [
    "dfTarget = pd.read_csv(\"./return_2004_40.csv\")\n",
    "dfTarget[\"return\"] = np.sign(dfTarget[\"return\"])\n",
    "# dfTarget.loc[dfTarget[\"return\"] == 0, \"return\"] = 1\n",
    "dfTarget = dfTarget[dfTarget.QAId.isin(chosen_stocks)]\n",
    "dfTarget = dfTarget[dfTarget.month_id >= (to_month_id(2004, 1) + lookback)]\n",
    "targets = MinMaxScaler().fit_transform(dfTarget.drop(columns=['month_id', 'QAId']).as_matrix())\n",
    "y = targets.reshape(157 - lookback + 1, len(chosen_stocks))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1525763882966,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "Vo9QHxamwmbL",
    "outputId": "9ef70b36-2a3c-44be-a6ed-22930b63500a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 264)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "splittedFeature = normalizedFeatures\n",
    "print(splittedFeature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2pHmaWhfZ4gm"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "x = np.zeros((157 - lookback + 1, lookback, n_features * len(chosen_stocks)))\n",
    "y_mock = np.zeros((157, len(chosen_stocks)))\n",
    "\n",
    "i = 0\n",
    "for train, test in TimeseriesGenerator(splittedFeature, y_mock, length=lookback, batch_size=1):\n",
    "  if i > 157 - lookback:\n",
    "    break\n",
    "  x[i] = train[0]\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1525763885182,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "SkuyoR1bZ699",
    "outputId": "d82f1d27-ee9e-4198-f364-67e97179fdaf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "validation_months = 24\n",
    "test_months = 26\n",
    "end_point = 157 - lookback + 1\n",
    "split_point = 157 - lookback + 1 - test_months\n",
    "\n",
    "x_train = x[0:split_point - validation_months + lookback].reshape(split_point - validation_months + lookback, lookback, n_features * len(chosen_stocks))\n",
    "y_train = y[0:split_point - validation_months + lookback].reshape(split_point - validation_months + lookback, len(chosen_stocks))\n",
    "x_validate = x[split_point - validation_months:split_point].reshape(validation_months, lookback, n_features * len(chosen_stocks))\n",
    "y_validate = y[split_point - validation_months:split_point].reshape(validation_months, len(chosen_stocks))\n",
    "x_test = x[split_point:end_point].reshape(test_months, lookback, n_features * len(chosen_stocks))\n",
    "y_test = y[split_point:end_point].reshape(test_months, len(chosen_stocks))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validate.shape)\n",
    "print(y_validate.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# ps = PredefinedSplit(np.append(np.negative(np.ones(60 * 465)), np.zeros(24 * 465))).split(x_train)\n",
    "\n",
    "# for train_ids, test_ids in ps:\n",
    "#   print(str(train_ids) + \", \" + str(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Co9Gsz7aZ_hb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Flatten, CuDNNLSTM\n",
    "from keras.regularizers import l1_l2, l2\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "\n",
    "np.random.seed(4103)\n",
    "\n",
    "def create_model(*param):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=1024, input_shape=(lookback, n_features * len(chosen_stocks)), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=1024, return_sequences=True, kernel_regularizer=l1_l2(0.0001)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=1024, return_sequences=False, kernel_regularizer=l1_l2(0.0001)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(512, activation=\"relu\", kernel_regularizer=l1_l2(0.0001)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(512, activation=\"relu\", kernel_regularizer=l1_l2(0.0001)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(len(chosen_stocks), activation=\"relu\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, decay=0.0), metrics=['accuracy'], *param)\n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13823,
     "status": "error",
     "timestamp": 1525764743477,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "0l18zd8xaQqR",
    "outputId": "47960b89-37a1-460d-aa9c-a58bbdff370f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 108 samples, validate on 24 samples\n",
      "Epoch 1/100\n",
      "108/108 [==============================] - 8s 79ms/step - loss: 65.5395 - acc: 0.4419 - val_loss: 63.7644 - val_acc: 0.3390\n",
      "Epoch 2/100\n",
      "108/108 [==============================] - 3s 31ms/step - loss: 62.9600 - acc: 0.4415 - val_loss: 62.2081 - val_acc: 0.3390\n",
      "Epoch 3/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 61.5607 - acc: 0.4491 - val_loss: 60.7092 - val_acc: 0.3845\n",
      "Epoch 4/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 60.3151 - acc: 0.4886 - val_loss: 59.6935 - val_acc: 0.5417\n",
      "Epoch 5/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 59.4170 - acc: 0.5303 - val_loss: 58.8306 - val_acc: 0.6193\n",
      "Epoch 6/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 58.5898 - acc: 0.5206 - val_loss: 57.9931 - val_acc: 0.6212\n",
      "Epoch 7/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 57.7204 - acc: 0.5358 - val_loss: 57.1719 - val_acc: 0.5928\n",
      "Epoch 8/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 56.8752 - acc: 0.5231 - val_loss: 56.3551 - val_acc: 0.5322\n",
      "Epoch 9/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 56.0301 - acc: 0.4954 - val_loss: 55.5373 - val_acc: 0.4943\n",
      "Epoch 10/100\n",
      "108/108 [==============================] - 4s 40ms/step - loss: 55.2001 - acc: 0.4949 - val_loss: 54.7152 - val_acc: 0.5133\n",
      "Epoch 11/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 54.3870 - acc: 0.5067 - val_loss: 53.8964 - val_acc: 0.5398\n",
      "Epoch 12/100\n",
      "108/108 [==============================] - 4s 40ms/step - loss: 53.5672 - acc: 0.5164 - val_loss: 53.0789 - val_acc: 0.5682\n",
      "Epoch 13/100\n",
      "108/108 [==============================] - 4s 41ms/step - loss: 52.7698 - acc: 0.5282 - val_loss: 52.2754 - val_acc: 0.6269\n",
      "Epoch 14/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 51.9867 - acc: 0.5261 - val_loss: 51.4852 - val_acc: 0.6307\n",
      "Epoch 15/100\n",
      "108/108 [==============================] - 4s 40ms/step - loss: 51.2088 - acc: 0.5240 - val_loss: 50.7112 - val_acc: 0.6420\n",
      "Epoch 16/100\n",
      "108/108 [==============================] - 4s 41ms/step - loss: 50.4268 - acc: 0.5164 - val_loss: 49.9518 - val_acc: 0.6439\n",
      "Epoch 17/100\n",
      "108/108 [==============================] - 4s 40ms/step - loss: 49.6566 - acc: 0.5471 - val_loss: 49.2037 - val_acc: 0.6345\n",
      "Epoch 18/100\n",
      "108/108 [==============================] - 4s 41ms/step - loss: 48.9124 - acc: 0.5278 - val_loss: 48.4645 - val_acc: 0.6212\n",
      "Epoch 19/100\n",
      "108/108 [==============================] - 4s 41ms/step - loss: 48.1645 - acc: 0.5560 - val_loss: 47.7336 - val_acc: 0.6212\n",
      "Epoch 20/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 47.4499 - acc: 0.5320 - val_loss: 47.0113 - val_acc: 0.6250\n",
      "Epoch 21/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 46.7277 - acc: 0.5404 - val_loss: 46.3012 - val_acc: 0.6269\n",
      "Epoch 22/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 46.0188 - acc: 0.5295 - val_loss: 45.5993 - val_acc: 0.6345\n",
      "Epoch 23/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 45.3218 - acc: 0.5560 - val_loss: 44.9112 - val_acc: 0.6383\n",
      "Epoch 24/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 44.6446 - acc: 0.5412 - val_loss: 44.2351 - val_acc: 0.6402\n",
      "Epoch 25/100\n",
      "108/108 [==============================] - 4s 41ms/step - loss: 43.9704 - acc: 0.5370 - val_loss: 43.5698 - val_acc: 0.6420\n",
      "Epoch 26/100\n",
      "108/108 [==============================] - 4s 34ms/step - loss: 43.3136 - acc: 0.5328 - val_loss: 42.9144 - val_acc: 0.6420\n",
      "Epoch 27/100\n",
      "108/108 [==============================] - 4s 34ms/step - loss: 42.6654 - acc: 0.5236 - val_loss: 42.2679 - val_acc: 0.6420\n",
      "Epoch 28/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 42.0107 - acc: 0.5417 - val_loss: 41.6348 - val_acc: 0.6402\n",
      "Epoch 29/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 41.3939 - acc: 0.5055 - val_loss: 41.0103 - val_acc: 0.6420\n",
      "Epoch 30/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 40.7789 - acc: 0.5143 - val_loss: 40.3939 - val_acc: 0.6477\n",
      "Epoch 31/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 40.1468 - acc: 0.5501 - val_loss: 39.7901 - val_acc: 0.6402\n",
      "Epoch 32/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 39.5591 - acc: 0.5042 - val_loss: 39.1956 - val_acc: 0.6383\n",
      "Epoch 33/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 38.9636 - acc: 0.5349 - val_loss: 38.6117 - val_acc: 0.6383\n",
      "Epoch 34/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 38.3688 - acc: 0.5564 - val_loss: 38.0329 - val_acc: 0.6383\n",
      "Epoch 35/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 37.8044 - acc: 0.5345 - val_loss: 37.4639 - val_acc: 0.6383\n",
      "Epoch 36/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 37.2432 - acc: 0.5400 - val_loss: 36.9074 - val_acc: 0.6364\n",
      "Epoch 37/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 36.6766 - acc: 0.5530 - val_loss: 36.3626 - val_acc: 0.6364\n",
      "Epoch 38/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 36.1479 - acc: 0.5290 - val_loss: 35.8278 - val_acc: 0.6364\n",
      "Epoch 39/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 35.6037 - acc: 0.5467 - val_loss: 35.2993 - val_acc: 0.6364\n",
      "Epoch 40/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 35.0840 - acc: 0.5379 - val_loss: 34.7734 - val_acc: 0.6458\n",
      "Epoch 41/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 34.5602 - acc: 0.5324 - val_loss: 34.2619 - val_acc: 0.6420\n",
      "Epoch 42/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 33.9762 - acc: 0.5412 - val_loss: 33.3254 - val_acc: 0.6439\n",
      "Epoch 43/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 33.2283 - acc: 0.5274 - val_loss: 32.7999 - val_acc: 0.6174\n",
      "Epoch 44/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 32.6777 - acc: 0.5404 - val_loss: 32.3345 - val_acc: 0.6080\n",
      "Epoch 45/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 32.2384 - acc: 0.5286 - val_loss: 31.8659 - val_acc: 0.5890\n",
      "Epoch 46/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 31.7465 - acc: 0.5332 - val_loss: 31.4030 - val_acc: 0.5455\n",
      "Epoch 47/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 31.2727 - acc: 0.5244 - val_loss: 30.9402 - val_acc: 0.5341\n",
      "Epoch 48/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 30.8170 - acc: 0.5076 - val_loss: 30.4677 - val_acc: 0.6591\n",
      "Epoch 49/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 30.3577 - acc: 0.5434 - val_loss: 30.0110 - val_acc: 0.6610\n",
      "Epoch 50/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 29.9120 - acc: 0.5556 - val_loss: 29.5735 - val_acc: 0.6648\n",
      "Epoch 51/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 29.4852 - acc: 0.5480 - val_loss: 29.1493 - val_acc: 0.6553\n",
      "Epoch 52/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 29.0585 - acc: 0.5370 - val_loss: 28.7388 - val_acc: 0.6572\n",
      "Epoch 53/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 28.6366 - acc: 0.5463 - val_loss: 28.3258 - val_acc: 0.6572\n",
      "Epoch 54/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 28.2225 - acc: 0.5434 - val_loss: 27.9185 - val_acc: 0.6572\n",
      "Epoch 55/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 27.8242 - acc: 0.5354 - val_loss: 27.5150 - val_acc: 0.6572\n",
      "Epoch 56/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 27.4295 - acc: 0.5434 - val_loss: 27.1250 - val_acc: 0.6572\n",
      "Epoch 57/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 27.0328 - acc: 0.5497 - val_loss: 26.7365 - val_acc: 0.6572\n",
      "Epoch 58/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 26.6530 - acc: 0.5513 - val_loss: 26.3524 - val_acc: 0.6572\n",
      "Epoch 59/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 26.2742 - acc: 0.5509 - val_loss: 25.9797 - val_acc: 0.6610\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 4s 38ms/step - loss: 25.9039 - acc: 0.5463 - val_loss: 25.6145 - val_acc: 0.6591\n",
      "Epoch 61/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 25.5430 - acc: 0.5118 - val_loss: 25.2588 - val_acc: 0.6572\n",
      "Epoch 62/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 25.1675 - acc: 0.5593 - val_loss: 24.9043 - val_acc: 0.6572\n",
      "Epoch 63/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 24.8212 - acc: 0.5636 - val_loss: 24.5504 - val_acc: 0.6591\n",
      "Epoch 64/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 24.4812 - acc: 0.5320 - val_loss: 24.2049 - val_acc: 0.6610\n",
      "Epoch 65/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 24.1374 - acc: 0.5598 - val_loss: 23.8692 - val_acc: 0.6629\n",
      "Epoch 66/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 23.8038 - acc: 0.5471 - val_loss: 23.5402 - val_acc: 0.6648\n",
      "Epoch 67/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 23.4760 - acc: 0.5459 - val_loss: 23.2180 - val_acc: 0.6610\n",
      "Epoch 68/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 23.1511 - acc: 0.5417 - val_loss: 22.8976 - val_acc: 0.6591\n",
      "Epoch 69/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 22.8328 - acc: 0.5581 - val_loss: 22.5823 - val_acc: 0.6591\n",
      "Epoch 70/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 22.5180 - acc: 0.5686 - val_loss: 22.2742 - val_acc: 0.6572\n",
      "Epoch 71/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 22.2072 - acc: 0.5593 - val_loss: 21.9665 - val_acc: 0.6610\n",
      "Epoch 72/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 21.9145 - acc: 0.5404 - val_loss: 21.6682 - val_acc: 0.6591\n",
      "Epoch 73/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 21.6124 - acc: 0.5421 - val_loss: 21.3748 - val_acc: 0.6572\n",
      "Epoch 74/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 21.3240 - acc: 0.5480 - val_loss: 21.0887 - val_acc: 0.6572\n",
      "Epoch 75/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 21.0269 - acc: 0.5779 - val_loss: 20.8043 - val_acc: 0.6610\n",
      "Epoch 76/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 20.7451 - acc: 0.5648 - val_loss: 20.5237 - val_acc: 0.6591\n",
      "Epoch 77/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 20.4729 - acc: 0.5526 - val_loss: 20.2483 - val_acc: 0.6610\n",
      "Epoch 78/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 20.2010 - acc: 0.5619 - val_loss: 19.9767 - val_acc: 0.6629\n",
      "Epoch 79/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 19.9339 - acc: 0.5593 - val_loss: 19.7168 - val_acc: 0.6572\n",
      "Epoch 80/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 19.6686 - acc: 0.5661 - val_loss: 19.4565 - val_acc: 0.6553\n",
      "Epoch 81/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 19.4104 - acc: 0.5539 - val_loss: 19.2007 - val_acc: 0.6572\n",
      "Epoch 82/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 19.1542 - acc: 0.5657 - val_loss: 18.9478 - val_acc: 0.6591\n",
      "Epoch 83/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 18.9056 - acc: 0.5547 - val_loss: 18.7005 - val_acc: 0.6610\n",
      "Epoch 84/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 18.6622 - acc: 0.5450 - val_loss: 18.4555 - val_acc: 0.6591\n",
      "Epoch 85/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 18.4100 - acc: 0.5678 - val_loss: 18.2174 - val_acc: 0.6610\n",
      "Epoch 86/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 18.1699 - acc: 0.5732 - val_loss: 17.9790 - val_acc: 0.6648\n",
      "Epoch 87/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 17.9445 - acc: 0.5585 - val_loss: 17.7487 - val_acc: 0.6591\n",
      "Epoch 88/100\n",
      "108/108 [==============================] - 4s 38ms/step - loss: 17.7088 - acc: 0.5699 - val_loss: 17.5225 - val_acc: 0.6610\n",
      "Epoch 89/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 17.4806 - acc: 0.5762 - val_loss: 17.2981 - val_acc: 0.6629\n",
      "Epoch 90/100\n",
      "108/108 [==============================] - 4s 39ms/step - loss: 17.2587 - acc: 0.5665 - val_loss: 17.0781 - val_acc: 0.6591\n",
      "Epoch 91/100\n",
      "108/108 [==============================] - 4s 35ms/step - loss: 17.0411 - acc: 0.5657 - val_loss: 16.8623 - val_acc: 0.6629\n",
      "Epoch 92/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 16.8380 - acc: 0.5425 - val_loss: 16.6487 - val_acc: 0.6648\n",
      "Epoch 93/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 16.6136 - acc: 0.5762 - val_loss: 16.4412 - val_acc: 0.6591\n",
      "Epoch 94/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 16.4090 - acc: 0.5665 - val_loss: 16.2400 - val_acc: 0.6610\n",
      "Epoch 95/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 16.2037 - acc: 0.5720 - val_loss: 16.0331 - val_acc: 0.6629\n",
      "Epoch 96/100\n",
      "108/108 [==============================] - 4s 37ms/step - loss: 16.0033 - acc: 0.5694 - val_loss: 15.8333 - val_acc: 0.6610\n",
      "Epoch 97/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 15.8099 - acc: 0.5522 - val_loss: 15.6374 - val_acc: 0.6610\n",
      "Epoch 98/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 15.6058 - acc: 0.5783 - val_loss: 15.4491 - val_acc: 0.6610\n",
      "Epoch 99/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 15.4151 - acc: 0.5787 - val_loss: 15.2576 - val_acc: 0.6629\n",
      "Epoch 100/100\n",
      "108/108 [==============================] - 4s 36ms/step - loss: 15.2335 - acc: 0.5770 - val_loss: 15.0745 - val_acc: 0.6610\n",
      "<keras.callbacks.History object at 0x1a499aecf8>\n"
     ]
    }
   ],
   "source": [
    "batch_size=36\n",
    "\n",
    "train_score = model.fit(x_train, y_train, validation_data=(x_validate, y_validate), batch_size=batch_size, epochs=100)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1157,
     "status": "error",
     "timestamp": 1525763095237,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "yycG9fQgaTWR",
    "outputId": "6bc04992-aa67-445c-a5dc-4034a301660a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 11ms/step\n",
      "loss: 15.115631103515625\n",
      "acc: 0.5839160680770874\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xd4VGX+/vH3Jz2QQEjoNTSRIoQQEASVtijYFQuKBQtrWV3X7+qy1dVturqKhUURRFEUEESRRQSpIggkiCBNREOXTihJSDJ5fn/MwA8VQoBMJjNzv65rrpk5c86Zz+FwzZ3nlOcx5xwiIhK+IgJdgIiIBJaCQEQkzCkIRETCnIJARCTMKQhERMKcgkBEJMwpCEREwpyCQOQ4ZpZtZr0DXYdIeVIQiIiEOQWBSCmY2T1m9q2Z7TWzKWZW1zfdzOx5M9tpZjlmtsLM2vg+62dmq83soJltNbPfBnYrRE5MQSByCmbWE/gXcANQB9gIjPN93Ae4CDgHSAJuBPb4PhsF/NI5lwi0AWaXY9kipRYV6AJEgsAtwOvOuWUAZvZ7YJ+ZpQKFQCJwLrDEObfmuOUKgVZm9pVzbh+wr1yrFikltQhETq0u3lYAAM65Q3j/6q/nnJsNvAwMA3aY2Qgzq+Kb9TqgH7DRzOaZWZdyrlukVBQEIqe2DWh09I2ZVQZSgK0AzrkXnXMdgNZ4DxE96pu+1Dl3FVAT+ACYUM51i5SKgkDk56LNLO7oA+8P+CAzSzOzWOCfwGLnXLaZdTSz880sGjgM5AMeM4sxs1vMrKpzrhA4AHgCtkUiJVAQiPzcNCDvuMeFwJ+BScB2oClwk2/eKsBreI//b8R7yOhZ32e3AtlmdgC4FxhYTvWLnBbTwDQiIuFNLQIRkTCnIBARCXMKAhGRMKcgEBEJc0FxZ3H16tVdampqoMsQEQkqWVlZu51zNU41X1AEQWpqKpmZmYEuQ0QkqJjZxlPPpUNDIiJhT0EgIhLmFAQiImEuKM4RnEhhYSFbtmwhPz8/0KWEjLi4OOrXr090dHSgSxGRchS0QbBlyxYSExNJTU3FzAJdTtBzzrFnzx62bNlC48aNA12OiJSjoD00lJ+fT0pKikKgjJgZKSkpamGJhKGgDQJAIVDG9O8pEp6COghOZX9uAXsOHQl0GSIiFVpIB0FOXiE7Dx7BH11t79mzh7S0NNLS0qhduzb16tU79r6goKBU6xg0aBDr1q0rcZ5hw4YxduzYsihZROSEgvZkcWlUiYsmJ6+QvEIPlWLKdlNTUlJYvnw5AH/9619JSEjgt7/97Y/mcc7hnCMi4sR5O3r06FN+zwMPPHD2xYqIlCCkWwSJcVEYcCCvqNy+89tvv6VNmzbce++9pKens337dgYPHkxGRgatW7fmySefPDZvt27dWL58OUVFRSQlJTFkyBDatWtHly5d2LlzJwB/+tOfGDp06LH5hwwZQqdOnWjRogULFy4E4PDhw1x33XW0a9eOAQMGkJGRcSykREROJSRaBE98tIrV2w6c8LP8Qg8OiI+OPK11tqpbhcevaH1G9axevZrRo0fzyiuvAPDUU0+RnJxMUVERPXr0oH///rRq1epHy+Tk5HDxxRfz1FNP8cgjj/D6668zZMiQn63bOceSJUuYMmUKTz75JNOnT+ell16idu3aTJo0ia+++or09PQzqltEwlNItwgAIiOM4mLnl/MEJ9O0aVM6dux47P27775Leno66enprFmzhtWrV/9smfj4ePr27QtAhw4dyM7OPuG6r7322p/Ns2DBAm66yTuEbrt27Wjd+swCTETCU0i0CEr6y/1IoYd1Ow5SNyme6gmx5VJP5cqVj71ev349L7zwAkuWLCEpKYmBAwee8Fr9mJiYY68jIyMpKjrx4azY2NifzaNxp0XkbIR2iyB3L7FH9hAbFcmBvMKAlHDgwAESExOpUqUK27dv55NPPinz7+jWrRsTJkwAYOXKlSdscYiInExItAhOKv8AHMkhKb4pOw958BQXE3mSK3j8JT09nVatWtGmTRuaNGlC165dy/w7HnzwQW677Tbatm1Leno6bdq0oWrVqmX+PSISmiwYDitkZGS4nw5Ms2bNGlq2bFnygoX5sGsNBXHVWZubSMPkSiRViil5mSBUVFREUVERcXFxrF+/nj59+rB+/Xqiok4/50v17yoiQcHMspxzGaeaL7RbBNFxEF+N6Ly9xEZU5kB+UUgGwaFDh+jVqxdFRUU453j11VfPKAREJDyF/q9FQm0sbx91ow6wMS+SgqJiYqJC69RIUlISWVlZgS5DRIJUaP0inkh0HMQnk1C0n0iK2HFAvWuKiBwv9IMAILEWhqNh9CH25RaQV1B+dxqLiFR04REEUd5WQaWifcRFFLMtJ1/X3ouI+IRHEAAk1saAhtEHOHykiIP5ahWIiEA4BUFULFRKIbZwPwlRHrbn5FN8Fq2C7t27/+zmsKFDh3L//fefdJmEhAQAtm3bRv/+/U+63p9eKvtTQ4cOJTc399j7fv36sX///tKWLiLyI+ETBOBrFRgNonI4UuRh18EzH7RmwIABjBs37kfTxo0bx4ABA065bN26dZk4ceIZf/dPg2DatGkkJSWd8fpEJLyFVxBERkNCDaILcqgR59h58Aj5hZ4zWlX//v2ZOnUqR454wyQ7O5tt27aRlpZGr169SE9P57zzzuPDDz/82bLZ2dm0adMGgLy8PG666Sbatm3LjTfeSF5e3rH57rvvvmPdVz/++OMAvPjii2zbto0ePXrQo0cPAFJTU9m9ezcAzz33HG3atKFNmzbHuq/Ozs6mZcuW3HPPPbRu3Zo+ffr86HtEJLyFxn0EHw+BH1aWcmYHBYepbZFUcdE4M1x0BMZPxuutfR70feqka0lJSaFTp05Mnz6dq666inHjxnHjjTcSHx/P5MmTqVKlCrt376Zz585ceeWVJx0PePjw4VSqVIkVK1awYsWKH3Uh/Y9//IPk5GQ8Hg+9evVixYoVPPTQQzz33HPMmTOH6tWr/2hdWVlZjB49msWLF+Oc4/zzz+fiiy+mWrVqrF+/nnfffZfXXnuNG264gUmTJjFw4MBS/puJSCjza4vAzJLMbKKZrTWzNWbWxcySzWymma33PVfzZw0nqAoiYzBXRFxEMZ5iR1HxmZ0rOP7w0NHDQs45/vCHP9C2bVt69+7N1q1b2bFjx0nXMX/+/GM/yG3btqVt27bHPpswYQLp6em0b9+eVatWnbIzuQULFnDNNddQuXJlEhISuPbaa/nss88AaNy4MWlpaUDJ3VyLSPjxd4vgBWC6c66/mcUAlYA/ALOcc0+Z2RBgCPC7s/qWEv5yPyHnYO8GIo4cYn9kfXI80TStkUDcaQ5ec/XVV/PII4+wbNky8vLySE9P54033mDXrl1kZWURHR1NamrqCbudPt6JWgvff/89zz77LEuXLqVatWrccccdp1xPSZfEHu2+GrxdWOvQkIgc5bcWgZlVAS4CRgE45wqcc/uBq4A3fbO9CVztrxpKKA6SUrHIaOq5H4iimOw9hyn0FJ/WahISEujevTt33nnnsZPEOTk51KxZk+joaObMmcPGjRtLXMdFF110bHD6r7/+mhUrVgDe7qsrV65M1apV2bFjBx9//PGxZRITEzl48OAJ1/XBBx+Qm5vL4cOHmTx5MhdeeOFpbZOIhB9/HhpqAuwCRpvZl2Y20swqA7Wcc9sBfM81T7SwmQ02s0wzy9y1a1fZVxcZBdVSseIimkXvpshTzMY9h/Gc5mGiAQMG8NVXXx0bIeyWW24hMzOTjIwMxo4dy7nnnlvi8vfddx+HDh2ibdu2/Pvf/6ZTp06Ad6Sx9u3b07p1a+68884fdV89ePBg+vbte+xk8VHp6enccccddOrUifPPP5+7776b9u3bn9b2iEj48Vs31GaWAXwBdHXOLTazF4ADwIPOuaTj5tvnnCvxPMEZd0NdGod3Q85mCmKqsS4/icS4aBqlVDrpyd1Qp26oRUJHabuh9meLYAuwxTm32Pd+IpAO7DCzOgC+551+rOHUKleHhFrEFOyjadxBDuQXsmVfnrqgEJGw4bcgcM79AGw2sxa+Sb2A1cAU4HbftNuBn19oX94S60ClFCoV7KZxXC77cgvYtl/9EYlIePD3VUMPAmN9Vwx9BwzCGz4TzOwuYBNw/Zmu3DlXNodwzKBqAyguIiF/Bw3j6rLpMEREQJ2q8We//iCh4BMJT34NAufccuBEx6d6ne264+Li2LNnDykpKWUXBkmp2N4NVC3YRr24emw9CFERRo3EuLNffwXnnGPPnj3ExYX+torIjwXtncX169dny5YtlPkVRa4YDuWAZxd5kdVYvimS5MrRVIoJ2n+qUouLi6N+/fqBLkNEylnQ/rpFR0fTuHFj/6w8dy+M7ofL2czjVf/OO1trMfL2DLq3OOGVriIiQS28Op0rrUrJcOtkrHINnjjwOH2r7+K+t5exbNO+QFcmIlLmFAQnU6UO3D4Fi03khcIn6Jiwk0Gjl7Luh5/f0SsiEswUBCVJagi3TyEiMorREX+nSeRObh21mM17c0+9rIhIkFAQnEpKU7jtQyKLC5kQ/y+Si3Zwy8jF7DxYcgdwIiLBQkFQGjVbwq2TiS44yAeJ/4ZDP3DbqCXk5BYGujIRkbOmICitumkwcBJx+bv5uNp/2LdrO3e+uZTcgqJAVyYiclYUBKejQUe4eTyVD29hZo3n2bBpM/e+vYyCotPrvlpEpCJREJyu1G5w09tUObiBWbVeYtk3G/nN+OWn3X21iEhFoSA4E816w/VvkJKzmpm1hzFrZTZDJq2gWGEgIkFIQXCmzr0MrnuNOjlf8UntV/kw63uenLpaHbeJSNAJ2i4mKoQ210FhHo0+fICPasdw2cK7qRwbyaOXlDwqmYhIRaIgOFvtB0JhHi2m/ZaJtWO4ds4dxEVF8mCv5oGuTESkVBQEZaHTPVBwmLRPH+ed2rEMmDmAuOhI7rmoSaArExE5JQVBWen2MBTm0nne04yuHcMd04zY6Ahu65Ia6MpEREqkIChL3X8PBYfpvuhlhteK5b4PIS46khsyGgS6MhGRk1IQlCUz6PN3KMyjb+Yo/lMzhkcnGfHRkVzRrm6gqxMROSEFQVkzg37PQlE+1y0fw+HqsTw83oiNiqBP69qBrk5E5GcUBP4QEQFXvgSFedy26jXykqN54B14ZWAHerWsFejqRER+REHgLxGRcO0IKMrnl+v+S361GO572xhxWwcNeSkiFYruLPanyGjoPxqa9uShQy8wOGkpg9/KYv43uwJdmYjIMQoCf4uOg5vewRpfyP/lPs/tVZZxz5hMhYGIVBgKgvIQHQ8DxmENOvOHvP8wsMpX3K0wEJEKQkFQXmIqwy0TsHod+FP+MwysupK7x2QyT2EgIgGmIChPsYkwcBJWtz1/znuaW6p+zT1jMpm7bmegKxORMKYgKG9xVbxhUCeNv+Q9zS1VVzF4TBZz1ioMRCQwFASBEFcVbn0fq9OWv+Q9xa3VVvLLt7KYtWZHoCsTkTCkIAiUuKpw62SsTjv+lPs0g6qt4N63s/h0tcJARMqXgiCQjoZBvQ4MOfw0dyd/xX1js5ipMBCRcuTXIDCzbDNbaWbLzSzTNy3ZzGaa2XrfczV/1lDhHT1n0KATjx36N/dVy+T+sVnMWPVDoCsTkTBRHi2CHs65NOdchu/9EGCWc645MMv3PrzFJsItE7FGXfnNoed4qNoX3D92GdO/VhiIiP8F4tDQVcCbvtdvAlcHoIaKJzYBbnkPa9qTBw+9wGPJ83ngnWX8b8X2QFcmIiHO353OOWCGmTngVefcCKCWc247gHNuu5mdsAc2MxsMDAZo2LChn8usIKLjYcC78N4dDF43nLjkAh4aBx7nuFLjGYiIn/g7CLo657b5fuxnmtna0i7oC40RABkZGc5fBVY4UbFwwxh4fzC3rRpFpeR8Hh5XTH6Bhxs6aqQzESl7fg0C59w23/NOM5sMdAJ2mFkdX2ugDqA7qX4qMhquGwkxlej/5dtUSsnn/klwuKCIQV0bB7o6EQkxfjtHYGaVzSzx6GugD/A1MAW43Tfb7cCH/qohqEVEwhUvQadf0u/QJN6q8Q5/++hrXp69HufCp4EkIv7nzxZBLWCymR39nnecc9PNbCkwwczuAjYB1/uxhuAWEQF9n4bYRC787Fkm1czj+hl3cLjAw2OXtMD3bysiclb8FgTOue+AdieYvgfo5a/vDTlm0OvPEFeF9jP/wtQa+Vw1dzC5R4p4/IrWREQoDETk7GioymDR9dcQm0iLqY8wo0Yuly/6FbkFHv517XlEReoGcRE5c/oFCSYZd2LXjaTh4a/5NOVZZmWt5v6xy8gv9AS6MhEJYgqCYHNef+ymd6l1JJs5yU+xas0qbnt9CTl5hYGuTESClIIgGJ3TB279gKqefXya9E8ObPqaG19dxM4D+YGuTESCkIIgWDXqAoP+R3xEMR8l/IOqe1fQ/5VFbNxzONCViUiQURAEs9rnwV2fEB1fhXdi/kGrvEyuG76I1dsOBLoyEQkiCoJgl9wE7ppBZEoThtvT9GMBN45YxOLv9gS6MhEJEgqCUJBYGwZNwxqcz5NFz3Nv7Cfc+voSdWMtIqWiIAgVcVVh4CRoeSUPHBnF01Xe44GxSxm7eGOgKxORCk5BEEqi4+D6N6Dj3VyTO4m3k0fz18nLeW7mN+qfSEROSncWh5qISOj3LFSpS5dZT/K/lByunXUvP+Tk8Y9rziNadyGLyE/oVyEUmcGF/wdXD6d53nJmJz/N3MyV3DMmk8NHigJdnYhUMAqCUJZ2M3bzBGoWbWdO0t/4Yf0ybnh1ETt045mIHEdBEOqa9YJBH1M52pha+W/U2v0FVw/7nLU/6F4DEfFSEISDOm3h7k+JqtaQUZFP0a9oFv2HL2LeN7sCXZmIVAAKgnBRtT7cOR1rfCF/9gzjj/GTuPONJbq8VEQUBGElrirc8h6k38aA/PG8kzSCJycv41/T1lBcrMtLRcJVqYLAzH5tZlXMa5SZLTOzPv4uTvwgMhqueBF6/5Xzc+fyafIzTJr/JfeNzSK3QFcUiYSj0rYI7nTOHcA7AH0NYBDwlN+qEv8yg26/gRvGUL/gO+YkPcnGNZn0H76IbfvzAl2diJSz0gbB0YFx+wGjnXNfHTdNglWrq7BB00iMckyt9CSpexdw1bDPWb55f6ArE5FyVNogyDKzGXiD4BMzSwSK/VeWlJt66XDPbKJqNGOYPc3tfMQNry7kw+VbA12ZiJST0nYxcReQBnznnMs1sxS8h4ckFFStB4M+xj64j1+tfoPWidv45bhbWPvDQR7t04KICDX+REJZaVsEVwEbnHNHjxl4gCb+KUkCIqYy9H8DLnqMHnkzmJH8LO/NXcbgt7I4pG4pREJaaYPgcedcztE3vkB43D8lScBEREDPP0L/0TQq+Ja5SU/wwzdL6D98IVv25Qa6OhHxk9IGwYnmU8+loarNtdid00mIieTDuCdos38WVw/7nKyNewNdmYj4QWmDINPMnjOzpmbWxMyeB7L8WZgEWN00GDyXyLppPMtQHrF3uHnEQt76YqPGNhAJMaUNggeBAmA88B6QDzzgr6KkgkioCbd/BB0GcXPh+7yXOJRnPviC3763gvxCT6CrE5EyYsHw111GRobLzMwMdBnhLXM0btqj5MTU5MacB4ms3YZXb+1Ag+RKga5MRE7CzLKccxmnmq/EFoGZDfU9f2RmU376KKtiJQhkDMIGTSMpysO0Sk/Qet+nXP7SAuau2xnoykTkLJ3qhO9bvudn/V2IBIEGneCX84iccDvPbB5K55hs7n4jn4d6t+RXPZrpfgORIFViEDjnsswsErjHOTfwTL7At3wmsNU5d7mZNQbGAcnAMuBW51zBmaxbAiCxtve8wYw/ct2SEbSrls1NM3/J11tzeO7GNBJidTGZSLA55cli55wHqGFmMWf4Hb8G1hz3/mngeedcc2Af3ruWJZhExUC/Z+CaV2lasJZ5Vf/CvnULuGbY52TvPhzo6kTkNJX2qqFs4HMz+7OZPXL0caqFzKw+cBkw0vfegJ7ARN8sbwJXn3bVUjG0uwm7ayaVKyUwPuZv9Dn4Ple8/BnTv94e6MpE5DSUNgi2AVN98yf6HgmlWG4o8Bj/v4O6FGC/c+5onwVbgHonWtDMBptZppll7tqlIRUrrDptYfBcIlpcyqPuDV6JeZFH317AX6es4kiRLjEVCQalPaC72jn33vETzOz6khYws8uBnb7zDN2PTj7BrCe8ftU5NwIYAd7LR0tZpwRCfBLc+DYsfJELPn2CuVU3cuui+7l+0z6G3ZyuS0xFKrjStgh+X8ppx+sKXGlm2XhPDvfE20JIMrOjAVQfb2tDgp0ZdP01dsf/SInx8FH8X+mw+0Mue3E+M1fvCHR1IlKCU91H0NfMXgLqmdmLxz3eAErsktI593vnXH3nXCpwEzDbOXcLMAfo75vtduDDs90IqUAadYF7FxDZuBuPM4IXY/7Lw2M+41/T1lDo0RAWIhXRqVoE2/Be+pmPt2+ho48pwCVn+J2/Ax4xs2/xnjMYdYbrkYqqcnW4ZRL0/BMXF37GvCqPs+Cz2QwY8QXbczQUpkhFU6ouJswsGu/5hIbOuXV+r+on1MVEEMv+HCbdhefwbv5VNJD3o/ox9Kb2XHROjUBXJhLyyqSLieNcCiwHpvtWnqYuJqRUUrt6DxU17cGfIkbzYsRzPDh6Nk99vJaCIh0qEqkIShsEfwU6AfsBnHPLgVT/lCQhp3J1GDAe+vydrsVLmZvwZ5bOn0b/VxbyvW5AEwm40gZB0fEjlImctogIuOBB7M4ZVEuI5724v3PJ7jFc8eI8JmRu1hgHIgFU2iD42sxuBiLNrLnvSqKFfqxLQlX9DvDLz4hocx0PMJ4Jcf/k+Ylz+NU7X5KTWxjo6kTC0ukMTNMaOAK8CxwAHvZXURLi4qrAda/BNa/Sku+Zk/AHItdMpu8L81n83Z5AVycSdjQwjQTW3u9g0j2wNZNPorrz6OGB3NGjLQ/1ak5UZGn/ThGREyntVUMlBsGprgxyzl15BrWdNgVBiPMUwvxncPOfYV9UTe49dA9FDbow9Mb2NExR9xQiZ6qsgmAXsBnv4aDF/KSvIOfcvLOss1QUBGFi8xJ4fzBuXzaj3RW84G7g0X5tueX8hng7rhWR01FW9xHUBv4AtAFeAH4B7HbOzSuvEJAw0qAT3LsA63A7d9oUPor9M+98OJVbRy1h637dkSziLyUGgXPO45yb7py7HegMfAvMNbMHy6U6CT+xCXDFC3DzBBrE5jI17i903DSSy4bOYfKXW3SZqYgfnPJsnJnFmtm1wNvAA8CLwPv+LkzC3DmXYPd/QUTrq/l1xAQmRv2FYRP+xwPvLGPfYY1sKlKWTtX76Jt47xdIB55wznV0zv3NObe1XKqT8FYpGfqPguvfoGnMXqbH/YlGa0dyyXNzNAqaSBk61cniYuBoHwDHz2iAc85V8WNtx+hksXBoJ0z9DaydyprIFjyQew+tzuvAE1e2JiUhNtDViVRIZXKy2DkX4ZxL9D2qHPdILK8QEAEgoaZ3FLTrRnFuzE5mxP+RemtGculzc/h4pVoHImdDd+xI8DCD8/pj9y8mqnlvfh85ljERj/PsOx/xwDvL2HPoSKArFAlKCgIJPom14KaxcM0Izo3ewYy4P9B0zXAuGzqHhRt2B7o6kaCjIJDgZAbtbsQeWEJky8t4JHICr7i/MWjk5wz99Bs8xbrMVKS0FAQS3BJqwg1vwpUvk+b5mjG13mXop99wx+glHD5S4rDaIuKjIJDQkH4rXPw7zt//MZPTlrFwwx5uHbWYnDx1bS1yKgoCCR0XD4FWV9N+7XO8130vK7fmcPNrX+gkssgpKAgkdEREwNXDoW570hc9yKdtZ7Np5176v7KIlVs0wJ7IySgIJLTEVILbPoT2t9JozWssSXmSxvlruea/nzNszrc6iSxyAgoCCT1xVeDKF2HgJOJdHqOKhvBGjbGM/GQpN7yq1oHITykIJHQ16w33L8I630/XAx+zOPExMna+z1Uvz+eR8cvZpq6tRQAFgYS6uKpw6T+x+z4npl47fu9eY2Hy39iych49/zOXV+ZtoNBTHOgqRQJKQSDhoWZLuP0j6D+a2pEHmRD1F0ZVHc3IjxdzxUsLWLZpX6ArFAkYBYGEDzNocy38ail0fZiuubNZlPAolxx8nxuHz+f376/UWAcSlhQEEn5iE+AXT8D9i4hO7cxvPKNZWPVxtmR9TM//zGXckk0U6+oiCSMKAglf1ZvDLRNhwDhqxDveiv4H/41+gZfen81Vwz4na+PeQFcoUi4UBBLezKBFX7h/MfT8E509Wcyr/Dsu2/82A4bP5+FxX/JDTn6gqxTxK78FgZnFmdkSM/vKzFaZ2RO+6Y3NbLGZrTez8WYW468aREotOg4uehR7MJOoFpdwr+ddFif9kYNfT6fnf+YybM63HCnyBLpKEb/wZ4vgCNDTOdcOSAMuNbPOwNPA88655sA+4C4/1iByeqrWhxvGwK2TqVY5jlFRT/FOwguMnzGfPs/PZ8aqHyhpeFeRYOS3IHBeh3xvo30PB/QEJvqmvwlc7a8aRM5Y055w30Lo9ThphV8xN/4x7i18i9+8tYCbX1vM6m0HAl2hSJnx6zkCM4s0s+XATmAmsAHY75w72lH8FqDeSZYdbGaZZpa5a9cuf5YpcmJRsXDhI/BgFhFtrmNAwSQyqzxGq+2TuPKluTz63ldsz9HdyRL8/BoEzjmPcy4NqA90AlqeaLaTLDvCOZfhnMuoUaOGP8sUKVmVOnDtq3D3bOJrNefPbgQLqz5OzldT6f7MHJ76eK3GPZCgVi5XDTnn9gNzgc5AkplF+T6qD2wrjxpEzlr9DnDndLjhLWpWghFR/+Z/VZ7mi/mfcPEzcxj52Xc6oSxByZ9XDdUwsyTf63igN7AGmAP09812O/Chv2oQKXNm0OpKeGAJ9HuWZraVD2L/wmuxQxk3bSa9/jOPD77cqhvSJKiYv66AMLO2eE8GR+INnAnOuSfNrAkwDkgGvgQGOueImpqdAAAQK0lEQVRKHEIqIyPDZWZm+qVOkbNy5CAsGgYLX8YVHGJWTA/+evBKEms3Y0jfc7moeXXMLNBVSpgysyznXMYp5wuGS+EUBFLhHd4Dnz+PW/IazlPE5Mhf8NShy2napCmPXXou6Q2rBbpCCUMKApFAOLAd5v8bt2wMHqJ4213K83n96NSqKf/X5xzOrV0l0BVKGFEQiATSng0w91+4lRMpiKzEyKLLGF5wCd3Pa8LDvZvTrGZioCuUMKAgEKkIdqyCOf+EtVPJjarK8ILLGFXYm95tm/BQr+Y0q5kQ6AolhCkIRCqSrVneQPj2Uw5FJ/PSkcsZU9iTS9ql8lCv5jSpoUCQsqcgEKmINn0Bs/8O2Z9xKDqFl49cxpjCHlya5m0hpFavHOgKJYQoCEQqsuwFMPcpbyBEJTOsoB9vFfXi0vZNub97U7UQpEwoCESCwcaF3kD4fh6Ho6rx3yN9ebOoNz3aNuGBHk11lZGcFQWBSDDZ9AXMexo2zCY3qiqvFVzKyIJf0KVVY37Vsxlt6ycFukIJQgoCkWC0eSnMfwbWf0J+ZAJvevrwSn4fzjvHe8jo/MbJulNZSk1BIBLMtn0Jn/0Ht2YqRRGxvOd68XLeJdRu2Iz7ujej17k1iYhQIEjJFAQioWDXOlgwFLdyAs7BjIhuPJfbl+IaLRl8UROuSqtLbFRkoKuUCkpBIBJK9m+GL/6Ly3oTKzzM0qgOPJd7Kd9Vbs/dFzbl5vMbUjk26tTrkbCiIBAJRbl7YelI3JIR2OFdfB/djOcPX8LC2G7c2rU5t3VpRLXKMYGuUioIBYFIKCvMhxXjYdHLsPsb9kTWYHh+HyZH9OKS9HO4s2tjdV8hCgKRsFBcDOtneAMh+zPyIyoztqg7Iwsupfk55zKoayoXN6+hE8thSkEgEm62LoNFw3CrJuMczLTO/Df/Eg5Wb8egC1K5rkN9KsXoPEI4URCIhKv9m2Dxq7hlb2JHDrImqiUv5/6CRTGdub5TE27t0oj61SoFukopBwoCkXB35CB8ORa3eDi2L5t9UdUZld+T8Z4epLc6h9u7pNKlaYpuUAthCgIR8Sr2wPqZsORV2DCbIotmuuvMqCO9OVQ9jdsuSOWa9Pok6PLTkKMgEJGf2/WN9/LT5WOxgkNsiGzKiPyezI66kL7pTRnYuRHn1NLoaaFCQSAiJ3fkIKwYj1s6Ctu5mryIBN4r6saYwp6kpLZlYOdGXNK6NjFREYGuVM6CgkBETs05b8+nmaNwqz/EPAUsj2jF6/k9WRrflas7NmFAx4Y0TNHJ5WCkIBCR03N4Nywfi8t8HduXzcGIqrxb0I13PT2p3+w8bu7UkN6tahEdqVZCsFAQiMiZKS6G7+ZA1mjc2mmY87DMWjPmyMVkVurGlRlNuUmthKCgIBCRs3fwB28rYdkYbF82hyMSmFh4AeOKepDcNJ3+HepzSevaulGtglIQiEjZKS6G7M9g2RjcmimYp4A1Ec0Ye+RCPo28kIvaNuOGjAZ0aFRN9yVUIAoCEfGP3L3eK46WjcF2rqbQYphZnMG7hRexPbkT/Tumck37etSqEhfoSsOegkBE/Ms52L7ce/fyyvew/P3siUhhfEFXPijuRq2maVyX7j10FB+jwXMCQUEgIuWn6Ais+xi+ehe3fibmPKy1Jowv6MrsqIu4oN259O/QgPSGSTp0VI4UBCISGId2wsqJuBXjsO1f4SGSz915TCzsxrqkC+mb3oSr0+qRWr1yoCsNeQEPAjNrAIwBagPFwAjn3AtmlgyMB1KBbOAG59y+ktalIBAJUjvXwIrxFH81noiD28izeKYVZfCBpyuH6lzAZWkNuKJdXZ1P8JOKEAR1gDrOuWVmlghkAVcDdwB7nXNPmdkQoJpz7nclrUtBIBLkioth4wJYMYHiVR8QUXCQfZbEB4XnM6W4K7GNOnJFWj36tamjoTbLUMCD4GdfZPYh8LLv0d05t90XFnOdcy1KWlZBIBJCCvNh/SewciLF33xChOcI26wWkwvP53+uKzWbpXN527r0aV2LKnHRga42qFWoIDCzVGA+0AbY5JxLOu6zfc65aiUtryAQCVH5ObDmI9zXk+C7eZjz8L3VZ0phR2a6LtRuns4VaXXp1bKWusk+AxUmCMwsAZgH/MM5976Z7S9NEJjZYGAwQMOGDTts3LjRr3WKSIAd2gWrP8Ct/gA2LsRcMdnUY0pRRz6lC3XO6UC/tnXpeW5NEtVSKJUKEQRmFg1MBT5xzj3nm7YOHRoSkZIc2glrpuBWfQAbP8dcMZuow9SijsykMynNOtL3vLr0blWLqvEKhZMJeBCY92LhN/GeGH74uOnPAHuOO1mc7Jx7rKR1KQhEwtihXbD2I9yqDyH7M8x52EZN/leUwad0Ir5xF/q0qcsvWtWiRmJsoKutUCpCEHQDPgNW4r18FOAPwGJgAtAQ2ARc75zbW9K6FAQiAni7t1j7P9yaj3Ab5hBRXMAeq8b0wnRmFGdwpH5XerSuT5/WtWms+xQCHwRlSUEgIj+TnwPrZ3pD4ZsZRBTlctgqMauoHTM8GWxK6cpFbZpwaZvatK5bJSzvaFYQiEj4KMyH7+bC2ql41k4jMm8PRUSxsLgln3rSWZVwAW3bnMcvWtaiY+PksBlcR0EgIuGp2AObF8O6aXjWTCNy3wYA1rhGzPS0Z2FUJ6o3O5+Lzq1F93NqUDOE72pWEIiIAOxeD+s+xrP2YyI2f4FRzB6SmFXUjtnF7dlf+wK6tGpCr5Y1Q+4QkoJAROSncvd6zyus/4Tibz4lsiCHIiLJLD6HOZ40VsZ3pEGLDHq0rEnXZtWD/n4FBYGISEk8RbBlCayfSdG6T4jatQqAH1wKsz1tWeDSyK3fjU4tU7moeQ1a1alCRERwtRYUBCIip+PANvj2U4q/mUHxhjlEFR7CQwRZxc2Z72nLirgMqjfvSNfmtbigWQp1qsYHuuJTUhCIiJwpTyFsXgIbZlG4bibRO1cAsI9EFnha81nxeWysej4tWrSke4sadGlSvUKOwqYgEBEpK4d3w4Y5uG8/pejb2UTn7gTge1eHzzxtWGLnUdCgK2nnpNK1aXXa1KtKZAU4jKQgEBHxB+dg11rYMBvPt3Ng4+dEFuVSjLGquBELi1uzPKotkaldyWhen67NqtOsZkJArkZSEIiIlIeiAtiaCd/Pp+DbeURuW0pkcSGFRLGsuBkLPa1ZHdeOyo3Pp2PzOpzfOIWmNSqXSzAoCEREAqEgFzZ/Ad/No2D9bKJ3rsRw5BNDpqc5XxS3Yk1cOyo36UTnZnW4oGkKjVIq+SUYFAQiIhVB3j7YuBD3/XwKNswndvdqAPKJ4UtPM5a4FnwX3464Jp1Ja1qPTo2TaVK9bFoMCgIRkYood683GLI/o+C7z4netYoIiikikpXFjVlc3JK1Ma1x9TvRsmkq/TvUp3rCmXWvXdog0NhvIiLlqVIytLwca3k5sQD5B2DzEiI3fs65386n7Y6PiSz+CDbB+ux6FNYdC83b+7UkBYGISCDFVYHmvbHmvYnvDRTmwdZlsPkLGn63kJgGTfxegoJARKQiiY6H1K6Q2pXYC/+vXL4yPDrlFhGRk1IQiIiEOQWBiEiYUxCIiIQ5BYGISJhTEIiIhDkFgYhImFMQiIiEuaDoa8jMdgEbz3Dx6sDuMiwnWITjdofjNkN4bre2uXQaOedqnGqmoAiCs2FmmaXpdCnUhON2h+M2Q3hut7a5bOnQkIhImFMQiIiEuXAIghGBLiBAwnG7w3GbITy3W9tchkL+HIGIiJQsHFoEIiJSAgWBiEiYC+kgMLNLzWydmX1rZkMCXY8/mFkDM5tjZmvMbJWZ/do3PdnMZprZet9ztUDXWtbMLNLMvjSzqb73jc1ssW+bx5tZTKBrLGtmlmRmE81srW+fdwn1fW1mv/H93/7azN41s7hQ3Ndm9rqZ7TSzr4+bdsJ9a14v+n7bVphZ+tl8d8gGgZlFAsOAvkArYICZtQpsVX5RBPyfc64l0Bl4wLedQ4BZzrnmwCzf+1Dza2DNce+fBp73bfM+4K6AVOVfLwDTnXPnAu3wbn/I7mszqwc8BGQ459oAkcBNhOa+fgO49CfTTrZv+wLNfY/BwPCz+eKQDQKgE/Ctc+4751wBMA64KsA1lTnn3Hbn3DLf64N4fxjq4d3WN32zvQlcHZgK/cPM6gOXASN97w3oCUz0zRKK21wFuAgYBeCcK3DO7SfE9zXeIXXjzSwKqARsJwT3tXNuPrD3J5NPtm+vAsY4ry+AJDOrc6bfHcpBUA/YfNz7Lb5pIcvMUoH2wGKglnNuO3jDAqgZuMr8YijwGFDse58C7HfOFfneh+L+bgLsAkb7DomNNLPKhPC+ds5tBZ4FNuENgBwgi9Df10edbN+W6e9bKAeBnWBayF4ra2YJwCTgYefcgUDX409mdjmw0zmXdfzkE8waavs7CkgHhjvn2gOHCaHDQCfiOyZ+FdAYqAtUxntY5KdCbV+fSpn+fw/lINgCNDjufX1gW4Bq8Sszi8YbAmOdc+/7Ju842lT0Pe8MVH1+0BW40syy8R7y64m3hZDkO3wAobm/twBbnHOLfe8n4g2GUN7XvYHvnXO7nHOFwPvABYT+vj7qZPu2TH/fQjkIlgLNfVcXxOA9wTQlwDWVOd+x8VHAGufcc8d9NAW43ff6duDD8q7NX5xzv3fO1XfOpeLdr7Odc7cAc4D+vtlCapsBnHM/AJvNrIVvUi9gNSG8r/EeEupsZpV8/9ePbnNI7+vjnGzfTgFu81091BnIOXoI6Yw450L2AfQDvgE2AH8MdD1+2sZueJuEK4Dlvkc/vMfMZwHrfc/Jga7VT9vfHZjqe90EWAJ8C7wHxAa6Pj9sbxqQ6dvfHwDVQn1fA08Aa4GvgbeA2FDc18C7eM+DFOL9i/+uk+1bvIeGhvl+21bivarqjL9bXUyIiIS5UD40JCIipaAgEBEJcwoCEZEwpyAQEQlzCgIRkTCnIBABzMxjZsuPe5TZHbtmlnp8j5IiFU3UqWcRCQt5zrm0QBchEghqEYiUwMyyzexpM1viezTzTW9kZrN8fcHPMrOGvum1zGyymX3le1zgW1Wkmb3m61d/hpnFB2yjRH5CQSDiFf+TQ0M3HvfZAedcJ+BlvH0a4Xs9xjnXFhgLvOib/iIwzznXDm8/QKt805sDw5xzrYH9wHV+3h6RUtOdxSKAmR1yziWcYHo20NM5952vc78fnHMpZrYbqOOcK/RN3+6cq25mu4D6zrkjx60jFZjpvIOLYGa/A6Kdc3/3/5aJnJpaBCKn5k7y+mTznMiR41570Pk5qUAUBCKnduNxz4t8rxfi7fkU4BZgge/1LOA+ODamcpXyKlLkTOmvEhGveDNbftz76c65o5eQxprZYrx/OA3wTXsIeN3MHsU7atgg3/RfAyPM7C68f/nfh7dHSZEKS+cIRErgO0eQ4ZzbHehaRPxFh4ZERMKcWgQiImFOLQIRkTCnIBARCXMKAhGRMKcgEBEJcwoCEZEw9/8AnQhHImMV63wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41142518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "for i, mn in enumerate(model.metrics_names):\n",
    "  print(mn + \": \" + str(test_score[i]))\n",
    "\n",
    "plt.plot(train_score.history[\"loss\"])\n",
    "plt.plot(train_score.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fYwRfk9VfdKR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "def threshold_tester(result, target, a, b):\n",
    "  for t in np.arange(a, b + 0.05, 0.05):\n",
    "    print(\"thresholder: \" + str(t))\n",
    "    print(classification_report(target, (result >= t).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2921
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "error",
     "timestamp": 1525762991314,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "zQ-ZbuKz3MVJ",
    "outputId": "12722ed0-2978-4350-8022-880def161a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6241998  0.58920246 0.59951013 0.5381867  0.59659874 0.6018089\n",
      "  0.60332596 0.54376924 0.59693044 0.56208456 0.66025513 0.47960275\n",
      "  0.56968504 0.54700416 0.58517784 0.5329204  0.57287234 0.57371104\n",
      "  0.6166224  0.6004933  0.60104    0.55080086]\n",
      " [0.6356464  0.6000288  0.6101079  0.5476919  0.607543   0.61222625\n",
      "  0.6138676  0.5527345  0.6079213  0.572114   0.6718963  0.48759013\n",
      "  0.58001274 0.55698776 0.59606165 0.541101   0.5829089  0.5828304\n",
      "  0.62774223 0.6108757  0.61196804 0.56023246]\n",
      " [0.6426261  0.6064425  0.61623925 0.55336154 0.61409557 0.618242\n",
      "  0.6201776  0.55796283 0.6144834  0.5780317  0.67878103 0.4923009\n",
      "  0.5862256  0.56295717 0.6025547  0.54584813 0.58891535 0.5878657\n",
      "  0.6344197  0.6169987  0.6185937  0.56577736]\n",
      " [0.6475467  0.6107789  0.62065965 0.5571815  0.6186019  0.6224816\n",
      "  0.62456346 0.5612708  0.61883    0.582064   0.6835095  0.49542108\n",
      "  0.59040964 0.56726706 0.6067136  0.5484913  0.5932414  0.591069\n",
      "  0.63925403 0.620873   0.6233333  0.5695767 ]\n",
      " [0.64523304 0.6075974  0.6175545  0.5542801  0.61563766 0.61972195\n",
      "  0.6217237  0.5581151  0.6158456  0.579503   0.68005276 0.49255872\n",
      "  0.58752966 0.56511736 0.6038939  0.5446731  0.590356   0.58753765\n",
      "  0.63656545 0.61718524 0.62101054 0.5666598 ]\n",
      " [0.6431413  0.6049672  0.61532736 0.5518539  0.6130881  0.6175017\n",
      "  0.6193556  0.555504   0.6132813  0.5772352  0.6772828  0.49041158\n",
      "  0.5850003  0.5635128  0.60134214 0.54129976 0.58814764 0.584443\n",
      "  0.63430357 0.61413926 0.61908776 0.5643949 ]\n",
      " [0.6423158  0.603791   0.6148275  0.55068326 0.6118213  0.6166836\n",
      "  0.61844796 0.55440295 0.61193675 0.57651    0.6760298  0.48940718\n",
      "  0.58383745 0.56333154 0.599827   0.53891885 0.58727795 0.58269197\n",
      "  0.6333916  0.6123239  0.6184901  0.5634067 ]\n",
      " [0.64188296 0.60327286 0.6145834  0.5504298  0.6116251  0.6165147\n",
      "  0.61838466 0.5540645  0.611349   0.5764784  0.6754942  0.48928502\n",
      "  0.58354557 0.5632745  0.5993479  0.53811353 0.5867555  0.58212674\n",
      "  0.6331589  0.6119913  0.61830175 0.56300414]\n",
      " [0.63572425 0.5970443  0.60892415 0.54480886 0.6054567  0.61089873\n",
      "  0.6127607  0.54881436 0.60531145 0.5710457  0.6689178  0.4848324\n",
      "  0.57795    0.55827236 0.59324497 0.53306675 0.5812351  0.5767059\n",
      "  0.6272446  0.6061887  0.61256254 0.5575748 ]\n",
      " [0.623373   0.5843212  0.5968409  0.5331409  0.5927236  0.5989719\n",
      "  0.6008283  0.5376153  0.5926769  0.5597556  0.6552378  0.47513163\n",
      "  0.5659721  0.54759073 0.5808011  0.52213126 0.5694099  0.5651345\n",
      "  0.614542   0.59366596 0.6006144  0.5462083 ]\n",
      " [0.6204627  0.5813513  0.59383696 0.53057367 0.58995795 0.5964534\n",
      "  0.59848285 0.535324   0.5899593  0.5573169  0.6522333  0.4733032\n",
      "  0.5635524  0.5448908  0.57821614 0.52018654 0.56639224 0.5626837\n",
      "  0.61171436 0.59130913 0.5978001  0.54349416]\n",
      " [0.6104614  0.570946   0.5838522  0.5210415  0.57954943 0.5867969\n",
      "  0.58862805 0.5261049  0.57942647 0.5477903  0.6411118  0.4652378\n",
      "  0.5538362  0.5360245  0.56808513 0.5114268  0.5566728  0.55324805\n",
      "  0.60134953 0.5809044  0.5879283  0.53410107]\n",
      " [0.60243976 0.5634576  0.57637846 0.51445675 0.5722649  0.57971513\n",
      "  0.58151364 0.5198248  0.5719719  0.5409612  0.63304967 0.45984763\n",
      "  0.54682374 0.52882683 0.5608749  0.505955   0.549564   0.54706395\n",
      "  0.59359217 0.57410747 0.580288   0.52758855]\n",
      " [0.59454197 0.5558629  0.5691968  0.50769055 0.5645014  0.5725274\n",
      "  0.5743024  0.51339304 0.56418914 0.53389883 0.6249399  0.45406234\n",
      "  0.5397402  0.52188194 0.55349046 0.5000925  0.5424921  0.5403742\n",
      "  0.58581626 0.56667316 0.57268924 0.5210352 ]\n",
      " [0.59082466 0.55244243 0.5658776  0.50474024 0.5612203  0.5692789\n",
      "  0.57121515 0.5105367  0.5608319  0.53086394 0.6215598  0.45179066\n",
      "  0.5367348  0.5185304  0.55027527 0.49763572 0.53929406 0.5375618\n",
      "  0.5823776  0.56354725 0.56910056 0.5181581 ]\n",
      " [0.5911614  0.55287284 0.56648546 0.5052034  0.5616905  0.5698568\n",
      "  0.5716946  0.5109593  0.5612224  0.5313019  0.6222369  0.45220336\n",
      "  0.5372287  0.5188575  0.55067044 0.49799916 0.53978425 0.53813434\n",
      "  0.58277375 0.5639358  0.56948537 0.51868474]\n",
      " [0.5902876  0.55197215 0.56578875 0.5043164  0.5608697  0.56918764\n",
      "  0.5709911  0.510059   0.5605555  0.53058636 0.6216888  0.45160764\n",
      "  0.53639823 0.51810896 0.5500246  0.49742848 0.5390952  0.5376482\n",
      "  0.58185565 0.5630801  0.5687978  0.51782066]\n",
      " [0.5882334  0.5499289  0.5637745  0.50228935 0.5587509  0.567346\n",
      "  0.5688875  0.508147   0.55861145 0.528612   0.61967576 0.44991046\n",
      "  0.534331   0.51607376 0.54806226 0.49566013 0.53712314 0.53574884\n",
      "  0.57957965 0.5609836  0.5667341  0.5159605 ]\n",
      " [0.5912461  0.5525239  0.56641936 0.5044253  0.5614013  0.56980693\n",
      "  0.5711647  0.5100391  0.5613572  0.5308694  0.6225885  0.45158893\n",
      "  0.536747   0.51863915 0.5507421  0.4973907  0.53963447 0.53785706\n",
      "  0.5823586  0.56334394 0.56944096 0.51812845]\n",
      " [0.5912906  0.55223495 0.5659836  0.5041931  0.56137425 0.56948805\n",
      "  0.5706671  0.5095619  0.56131315 0.53056365 0.62232435 0.45103824\n",
      "  0.5365996  0.51857126 0.550782   0.4970267  0.53934705 0.53750914\n",
      "  0.5820532  0.56305635 0.56944096 0.51781034]\n",
      " [0.5901917  0.5507203  0.5645851  0.5025493  0.5598973  0.5680098\n",
      "  0.5688783  0.5078475  0.5598519  0.5288683  0.62053174 0.44933718\n",
      "  0.53507733 0.51758    0.54934245 0.4951015  0.53793186 0.5357428\n",
      "  0.58049154 0.56102836 0.5681516  0.5162961 ]\n",
      " [0.5921249  0.5520904  0.5659628  0.50353897 0.561356   0.56921244\n",
      "  0.56991136 0.50856256 0.5612912  0.5299437  0.62168896 0.44969693\n",
      "  0.5364301  0.5191766  0.55086094 0.49563056 0.53905773 0.5365122\n",
      "  0.5819598  0.5618943  0.5696708  0.51703864]\n",
      " [0.59305716 0.552365   0.5664143  0.5036148  0.5618492  0.5694591\n",
      "  0.56983626 0.5083109  0.5616917  0.53001267 0.62173927 0.44944447\n",
      "  0.53661966 0.51986563 0.5513832  0.4953966  0.53921044 0.5364853\n",
      "  0.58239233 0.5620586  0.57029676 0.51694155]\n",
      " [0.5986209  0.557236   0.5714979  0.50795543 0.56679595 0.5743295\n",
      "  0.5744667  0.512562   0.56655926 0.53446126 0.6269208  0.4531217\n",
      "  0.54124916 0.52476573 0.5563174  0.49925777 0.54379576 0.54085165\n",
      "  0.5875801  0.56677395 0.5753082  0.5213186 ]\n",
      " [0.6044691  0.5625914  0.57700336 0.5128449  0.57232755 0.5795507\n",
      "  0.5798042  0.517113   0.5720121  0.53939384 0.6327075  0.45721176\n",
      "  0.5464487  0.52987087 0.56179637 0.5036855  0.5488879  0.5456083\n",
      "  0.59338766 0.5720721  0.58081806 0.5258401 ]\n",
      " [0.62352175 0.5760285  0.5905795  0.5248862  0.58741164 0.59192973\n",
      "  0.58868086 0.52485687 0.5826182  0.5523072  0.6432805  0.45932934\n",
      "  0.55687654 0.5479867  0.5724849  0.50239325 0.5613522  0.5538479\n",
      "  0.6083918  0.57168436 0.5998261  0.53885263]]\n",
      "thresholder: 0.1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.15000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.20000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.25000000000000006\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.30000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.3500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.40000000000000013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.38      1.00      0.56        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      1.00      0.74       330\n",
      "\n",
      "thresholder: 0.45000000000000007\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.32      0.70      0.44        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.58      1.00      0.73        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.59      0.99      0.73       330\n",
      "\n",
      "thresholder: 0.5000000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.50      1.00      0.67        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.58      1.00      0.73        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.73      1.00      0.84        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.46      1.00      0.63        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.62      1.00      0.76        16\n",
      "         13       0.58      1.00      0.73        15\n",
      "         14       0.62      1.00      0.76        16\n",
      "         15       0.56      0.60      0.58        15\n",
      "         16       0.62      1.00      0.76        16\n",
      "         17       0.69      1.00      0.82        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.46      1.00      0.63        12\n",
      "\n",
      "avg / total       0.58      0.95      0.71       330\n",
      "\n",
      "thresholder: 0.5500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      1.00      0.70        14\n",
      "          1       0.48      0.92      0.63        13\n",
      "          2       0.62      1.00      0.76        16\n",
      "          3       0.67      0.27      0.38        15\n",
      "          4       0.58      1.00      0.73        15\n",
      "          5       0.65      1.00      0.79        17\n",
      "          6       0.54      1.00      0.70        14\n",
      "          7       0.57      0.21      0.31        19\n",
      "          8       0.58      1.00      0.73        15\n",
      "          9       0.50      0.50      0.50        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.54      0.44      0.48        16\n",
      "         13       0.38      0.20      0.26        15\n",
      "         14       0.62      0.94      0.75        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.54      0.44      0.48        16\n",
      "         17       0.62      0.44      0.52        18\n",
      "         18       0.69      1.00      0.82        18\n",
      "         19       0.58      1.00      0.73        15\n",
      "         20       0.58      1.00      0.73        15\n",
      "         21       0.56      0.42      0.48        12\n",
      "\n",
      "avg / total       0.53      0.68      0.56       330\n",
      "\n",
      "thresholder: 0.6000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.43      0.41        14\n",
      "          1       0.29      0.15      0.20        13\n",
      "          2       0.38      0.19      0.25        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.38      0.20      0.26        15\n",
      "          5       0.67      0.35      0.46        17\n",
      "          6       0.30      0.21      0.25        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.50      0.27      0.35        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.54      1.00      0.70        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.75      0.19      0.30        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.54      0.39      0.45        18\n",
      "         19       0.89      0.53      0.67        15\n",
      "         20       0.70      0.47      0.56        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.29      0.20      0.22       330\n",
      "\n",
      "thresholder: 0.6500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.73      0.57      0.64        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.03      0.02      0.03       330\n",
      "\n",
      "thresholder: 0.7000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.00      0.00      0.00        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00       330\n",
      "\n",
      "thresholder: 0.7500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.00      0.00      0.00        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00       330\n",
      "\n",
      "thresholder: 0.8000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.00      0.00      0.00        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00       330\n",
      "\n",
      "thresholder: 0.8500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.00      0.00      0.00        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00       330\n",
      "\n",
      "thresholder: 0.9000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        14\n",
      "          1       0.00      0.00      0.00        13\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        15\n",
      "          4       0.00      0.00      0.00        15\n",
      "          5       0.00      0.00      0.00        17\n",
      "          6       0.00      0.00      0.00        14\n",
      "          7       0.00      0.00      0.00        19\n",
      "          8       0.00      0.00      0.00        15\n",
      "          9       0.00      0.00      0.00        12\n",
      "         10       0.00      0.00      0.00        14\n",
      "         11       0.00      0.00      0.00        10\n",
      "         12       0.00      0.00      0.00        16\n",
      "         13       0.00      0.00      0.00        15\n",
      "         14       0.00      0.00      0.00        16\n",
      "         15       0.00      0.00      0.00        15\n",
      "         16       0.00      0.00      0.00        16\n",
      "         17       0.00      0.00      0.00        18\n",
      "         18       0.00      0.00      0.00        18\n",
      "         19       0.00      0.00      0.00        15\n",
      "         20       0.00      0.00      0.00        15\n",
      "         21       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.00      0.00      0.00       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billykwok/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(x_test, batch_size=batch_size)\n",
    "print(result)\n",
    "threshold_tester(result, y_test, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1525762727684,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "qcA2GSqdINA-",
    "outputId": "b09b9962-ff00-4a8f-c628-e0e0f9877b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "ADM\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "AFL\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "AIG\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "ALL\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "AMZN\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "AXP\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "COF\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "\n",
      "\n",
      "COST\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "CVS\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "DIS\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "F\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "\n",
      "\n",
      "FDX\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "HAL\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "HIG\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "LOW\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "\n",
      "\n",
      "MET\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t0\tNothing\n",
      "\n",
      "\n",
      "PGR\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "\n",
      "\n",
      "PRU\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "SBUX\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "\n",
      "\n",
      "SPC\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "WAG\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "\n",
      "\n",
      "[]\n",
      "[{'month_id': 235, 'QAId': 'AAPL'}, {'month_id': 236, 'QAId': 'AAPL'}, {'month_id': 237, 'QAId': 'AAPL'}, {'month_id': 238, 'QAId': 'AAPL'}, {'month_id': 239, 'QAId': 'AAPL'}, {'month_id': 240, 'QAId': 'AAPL'}, {'month_id': 241, 'QAId': 'AAPL'}, {'month_id': 242, 'QAId': 'AAPL'}, {'month_id': 243, 'QAId': 'AAPL'}, {'month_id': 244, 'QAId': 'AAPL'}, {'month_id': 245, 'QAId': 'AAPL'}, {'month_id': 246, 'QAId': 'AAPL'}, {'month_id': 247, 'QAId': 'AAPL'}, {'month_id': 234, 'QAId': 'ADM'}, {'month_id': 235, 'QAId': 'ADM'}, {'month_id': 236, 'QAId': 'ADM'}, {'month_id': 237, 'QAId': 'ADM'}, {'month_id': 238, 'QAId': 'ADM'}, {'month_id': 239, 'QAId': 'ADM'}, {'month_id': 240, 'QAId': 'ADM'}, {'month_id': 241, 'QAId': 'ADM'}, {'month_id': 242, 'QAId': 'ADM'}, {'month_id': 243, 'QAId': 'ADM'}, {'month_id': 244, 'QAId': 'ADM'}, {'month_id': 245, 'QAId': 'ADM'}, {'month_id': 246, 'QAId': 'ADM'}, {'month_id': 247, 'QAId': 'ADM'}, {'month_id': 235, 'QAId': 'AFL'}, {'month_id': 236, 'QAId': 'AFL'}, {'month_id': 237, 'QAId': 'AFL'}, {'month_id': 238, 'QAId': 'AFL'}, {'month_id': 239, 'QAId': 'AFL'}, {'month_id': 240, 'QAId': 'AFL'}, {'month_id': 241, 'QAId': 'AFL'}, {'month_id': 242, 'QAId': 'AFL'}, {'month_id': 243, 'QAId': 'AFL'}, {'month_id': 244, 'QAId': 'AFL'}, {'month_id': 245, 'QAId': 'AFL'}, {'month_id': 246, 'QAId': 'AFL'}, {'month_id': 247, 'QAId': 'AFL'}, {'month_id': 234, 'QAId': 'AIG'}, {'month_id': 235, 'QAId': 'AIG'}, {'month_id': 236, 'QAId': 'AIG'}, {'month_id': 237, 'QAId': 'AIG'}, {'month_id': 238, 'QAId': 'AIG'}, {'month_id': 239, 'QAId': 'AIG'}, {'month_id': 240, 'QAId': 'AIG'}, {'month_id': 241, 'QAId': 'AIG'}, {'month_id': 242, 'QAId': 'AIG'}, {'month_id': 243, 'QAId': 'AIG'}, {'month_id': 244, 'QAId': 'AIG'}, {'month_id': 245, 'QAId': 'AIG'}, {'month_id': 246, 'QAId': 'AIG'}, {'month_id': 247, 'QAId': 'AIG'}, {'month_id': 234, 'QAId': 'ALL'}, {'month_id': 235, 'QAId': 'ALL'}, {'month_id': 236, 'QAId': 'ALL'}, {'month_id': 237, 'QAId': 'ALL'}, {'month_id': 238, 'QAId': 'ALL'}, {'month_id': 239, 'QAId': 'ALL'}, {'month_id': 240, 'QAId': 'ALL'}, {'month_id': 241, 'QAId': 'ALL'}, {'month_id': 242, 'QAId': 'ALL'}, {'month_id': 243, 'QAId': 'ALL'}, {'month_id': 244, 'QAId': 'ALL'}, {'month_id': 245, 'QAId': 'ALL'}, {'month_id': 246, 'QAId': 'ALL'}, {'month_id': 247, 'QAId': 'ALL'}, {'month_id': 235, 'QAId': 'AMZN'}, {'month_id': 236, 'QAId': 'AMZN'}, {'month_id': 237, 'QAId': 'AMZN'}, {'month_id': 238, 'QAId': 'AMZN'}, {'month_id': 239, 'QAId': 'AMZN'}, {'month_id': 240, 'QAId': 'AMZN'}, {'month_id': 241, 'QAId': 'AMZN'}, {'month_id': 242, 'QAId': 'AMZN'}, {'month_id': 243, 'QAId': 'AMZN'}, {'month_id': 244, 'QAId': 'AMZN'}, {'month_id': 245, 'QAId': 'AMZN'}, {'month_id': 246, 'QAId': 'AMZN'}, {'month_id': 247, 'QAId': 'AMZN'}, {'month_id': 235, 'QAId': 'AXP'}, {'month_id': 236, 'QAId': 'AXP'}, {'month_id': 237, 'QAId': 'AXP'}, {'month_id': 238, 'QAId': 'AXP'}, {'month_id': 239, 'QAId': 'AXP'}, {'month_id': 240, 'QAId': 'AXP'}, {'month_id': 241, 'QAId': 'AXP'}, {'month_id': 242, 'QAId': 'AXP'}, {'month_id': 243, 'QAId': 'AXP'}, {'month_id': 244, 'QAId': 'AXP'}, {'month_id': 245, 'QAId': 'AXP'}, {'month_id': 246, 'QAId': 'AXP'}, {'month_id': 247, 'QAId': 'AXP'}, {'month_id': 234, 'QAId': 'COF'}, {'month_id': 235, 'QAId': 'COF'}, {'month_id': 236, 'QAId': 'COF'}, {'month_id': 237, 'QAId': 'COF'}, {'month_id': 238, 'QAId': 'COF'}, {'month_id': 239, 'QAId': 'COF'}, {'month_id': 240, 'QAId': 'COF'}, {'month_id': 241, 'QAId': 'COF'}, {'month_id': 242, 'QAId': 'COF'}, {'month_id': 243, 'QAId': 'COF'}, {'month_id': 244, 'QAId': 'COF'}, {'month_id': 245, 'QAId': 'COF'}, {'month_id': 246, 'QAId': 'COF'}, {'month_id': 247, 'QAId': 'COF'}, {'month_id': 248, 'QAId': 'COF'}, {'month_id': 234, 'QAId': 'COST'}, {'month_id': 235, 'QAId': 'COST'}, {'month_id': 236, 'QAId': 'COST'}, {'month_id': 237, 'QAId': 'COST'}, {'month_id': 238, 'QAId': 'COST'}, {'month_id': 239, 'QAId': 'COST'}, {'month_id': 240, 'QAId': 'COST'}, {'month_id': 241, 'QAId': 'COST'}, {'month_id': 242, 'QAId': 'COST'}, {'month_id': 243, 'QAId': 'COST'}, {'month_id': 244, 'QAId': 'COST'}, {'month_id': 245, 'QAId': 'COST'}, {'month_id': 246, 'QAId': 'COST'}, {'month_id': 247, 'QAId': 'COST'}, {'month_id': 235, 'QAId': 'CVS'}, {'month_id': 236, 'QAId': 'CVS'}, {'month_id': 237, 'QAId': 'CVS'}, {'month_id': 238, 'QAId': 'CVS'}, {'month_id': 239, 'QAId': 'CVS'}, {'month_id': 240, 'QAId': 'CVS'}, {'month_id': 241, 'QAId': 'CVS'}, {'month_id': 242, 'QAId': 'CVS'}, {'month_id': 243, 'QAId': 'CVS'}, {'month_id': 244, 'QAId': 'CVS'}, {'month_id': 245, 'QAId': 'CVS'}, {'month_id': 246, 'QAId': 'CVS'}, {'month_id': 247, 'QAId': 'CVS'}, {'month_id': 234, 'QAId': 'DIS'}, {'month_id': 235, 'QAId': 'DIS'}, {'month_id': 236, 'QAId': 'DIS'}, {'month_id': 237, 'QAId': 'DIS'}, {'month_id': 238, 'QAId': 'DIS'}, {'month_id': 239, 'QAId': 'DIS'}, {'month_id': 240, 'QAId': 'DIS'}, {'month_id': 241, 'QAId': 'DIS'}, {'month_id': 242, 'QAId': 'DIS'}, {'month_id': 243, 'QAId': 'DIS'}, {'month_id': 244, 'QAId': 'DIS'}, {'month_id': 245, 'QAId': 'DIS'}, {'month_id': 246, 'QAId': 'DIS'}, {'month_id': 247, 'QAId': 'DIS'}, {'month_id': 235, 'QAId': 'F'}, {'month_id': 236, 'QAId': 'F'}, {'month_id': 237, 'QAId': 'F'}, {'month_id': 238, 'QAId': 'F'}, {'month_id': 239, 'QAId': 'F'}, {'month_id': 240, 'QAId': 'F'}, {'month_id': 241, 'QAId': 'F'}, {'month_id': 242, 'QAId': 'F'}, {'month_id': 243, 'QAId': 'F'}, {'month_id': 244, 'QAId': 'F'}, {'month_id': 245, 'QAId': 'F'}, {'month_id': 246, 'QAId': 'F'}, {'month_id': 247, 'QAId': 'F'}, {'month_id': 248, 'QAId': 'F'}, {'month_id': 234, 'QAId': 'FDX'}, {'month_id': 235, 'QAId': 'FDX'}, {'month_id': 236, 'QAId': 'FDX'}, {'month_id': 237, 'QAId': 'FDX'}, {'month_id': 238, 'QAId': 'FDX'}, {'month_id': 239, 'QAId': 'FDX'}, {'month_id': 240, 'QAId': 'FDX'}, {'month_id': 241, 'QAId': 'FDX'}, {'month_id': 242, 'QAId': 'FDX'}, {'month_id': 243, 'QAId': 'FDX'}, {'month_id': 244, 'QAId': 'FDX'}, {'month_id': 245, 'QAId': 'FDX'}, {'month_id': 246, 'QAId': 'FDX'}, {'month_id': 247, 'QAId': 'FDX'}, {'month_id': 235, 'QAId': 'HAL'}, {'month_id': 236, 'QAId': 'HAL'}, {'month_id': 237, 'QAId': 'HAL'}, {'month_id': 238, 'QAId': 'HAL'}, {'month_id': 239, 'QAId': 'HAL'}, {'month_id': 240, 'QAId': 'HAL'}, {'month_id': 241, 'QAId': 'HAL'}, {'month_id': 242, 'QAId': 'HAL'}, {'month_id': 243, 'QAId': 'HAL'}, {'month_id': 244, 'QAId': 'HAL'}, {'month_id': 245, 'QAId': 'HAL'}, {'month_id': 246, 'QAId': 'HAL'}, {'month_id': 247, 'QAId': 'HAL'}, {'month_id': 234, 'QAId': 'HIG'}, {'month_id': 235, 'QAId': 'HIG'}, {'month_id': 236, 'QAId': 'HIG'}, {'month_id': 237, 'QAId': 'HIG'}, {'month_id': 238, 'QAId': 'HIG'}, {'month_id': 239, 'QAId': 'HIG'}, {'month_id': 240, 'QAId': 'HIG'}, {'month_id': 241, 'QAId': 'HIG'}, {'month_id': 242, 'QAId': 'HIG'}, {'month_id': 243, 'QAId': 'HIG'}, {'month_id': 244, 'QAId': 'HIG'}, {'month_id': 245, 'QAId': 'HIG'}, {'month_id': 246, 'QAId': 'HIG'}, {'month_id': 247, 'QAId': 'HIG'}, {'month_id': 234, 'QAId': 'LOW'}, {'month_id': 235, 'QAId': 'LOW'}, {'month_id': 236, 'QAId': 'LOW'}, {'month_id': 237, 'QAId': 'LOW'}, {'month_id': 238, 'QAId': 'LOW'}, {'month_id': 239, 'QAId': 'LOW'}, {'month_id': 240, 'QAId': 'LOW'}, {'month_id': 241, 'QAId': 'LOW'}, {'month_id': 242, 'QAId': 'LOW'}, {'month_id': 243, 'QAId': 'LOW'}, {'month_id': 244, 'QAId': 'LOW'}, {'month_id': 245, 'QAId': 'LOW'}, {'month_id': 246, 'QAId': 'LOW'}, {'month_id': 247, 'QAId': 'LOW'}, {'month_id': 248, 'QAId': 'LOW'}, {'month_id': 234, 'QAId': 'MET'}, {'month_id': 235, 'QAId': 'MET'}, {'month_id': 236, 'QAId': 'MET'}, {'month_id': 237, 'QAId': 'MET'}, {'month_id': 238, 'QAId': 'MET'}, {'month_id': 239, 'QAId': 'MET'}, {'month_id': 240, 'QAId': 'MET'}, {'month_id': 241, 'QAId': 'MET'}, {'month_id': 242, 'QAId': 'MET'}, {'month_id': 243, 'QAId': 'MET'}, {'month_id': 244, 'QAId': 'MET'}, {'month_id': 245, 'QAId': 'MET'}, {'month_id': 246, 'QAId': 'MET'}, {'month_id': 247, 'QAId': 'MET'}, {'month_id': 234, 'QAId': 'PGR'}, {'month_id': 235, 'QAId': 'PGR'}, {'month_id': 236, 'QAId': 'PGR'}, {'month_id': 237, 'QAId': 'PGR'}, {'month_id': 238, 'QAId': 'PGR'}, {'month_id': 239, 'QAId': 'PGR'}, {'month_id': 240, 'QAId': 'PGR'}, {'month_id': 241, 'QAId': 'PGR'}, {'month_id': 242, 'QAId': 'PGR'}, {'month_id': 243, 'QAId': 'PGR'}, {'month_id': 244, 'QAId': 'PGR'}, {'month_id': 245, 'QAId': 'PGR'}, {'month_id': 246, 'QAId': 'PGR'}, {'month_id': 247, 'QAId': 'PGR'}, {'month_id': 248, 'QAId': 'PGR'}, {'month_id': 235, 'QAId': 'PRU'}, {'month_id': 236, 'QAId': 'PRU'}, {'month_id': 237, 'QAId': 'PRU'}, {'month_id': 238, 'QAId': 'PRU'}, {'month_id': 239, 'QAId': 'PRU'}, {'month_id': 240, 'QAId': 'PRU'}, {'month_id': 241, 'QAId': 'PRU'}, {'month_id': 242, 'QAId': 'PRU'}, {'month_id': 243, 'QAId': 'PRU'}, {'month_id': 244, 'QAId': 'PRU'}, {'month_id': 245, 'QAId': 'PRU'}, {'month_id': 246, 'QAId': 'PRU'}, {'month_id': 247, 'QAId': 'PRU'}, {'month_id': 234, 'QAId': 'SBUX'}, {'month_id': 235, 'QAId': 'SBUX'}, {'month_id': 236, 'QAId': 'SBUX'}, {'month_id': 237, 'QAId': 'SBUX'}, {'month_id': 238, 'QAId': 'SBUX'}, {'month_id': 239, 'QAId': 'SBUX'}, {'month_id': 240, 'QAId': 'SBUX'}, {'month_id': 241, 'QAId': 'SBUX'}, {'month_id': 242, 'QAId': 'SBUX'}, {'month_id': 243, 'QAId': 'SBUX'}, {'month_id': 244, 'QAId': 'SBUX'}, {'month_id': 245, 'QAId': 'SBUX'}, {'month_id': 246, 'QAId': 'SBUX'}, {'month_id': 247, 'QAId': 'SBUX'}, {'month_id': 248, 'QAId': 'SBUX'}, {'month_id': 235, 'QAId': 'SPC'}, {'month_id': 236, 'QAId': 'SPC'}, {'month_id': 237, 'QAId': 'SPC'}, {'month_id': 238, 'QAId': 'SPC'}, {'month_id': 239, 'QAId': 'SPC'}, {'month_id': 240, 'QAId': 'SPC'}, {'month_id': 241, 'QAId': 'SPC'}, {'month_id': 242, 'QAId': 'SPC'}, {'month_id': 243, 'QAId': 'SPC'}, {'month_id': 244, 'QAId': 'SPC'}, {'month_id': 245, 'QAId': 'SPC'}, {'month_id': 246, 'QAId': 'SPC'}, {'month_id': 247, 'QAId': 'SPC'}, {'month_id': 234, 'QAId': 'WAG'}, {'month_id': 235, 'QAId': 'WAG'}, {'month_id': 236, 'QAId': 'WAG'}, {'month_id': 237, 'QAId': 'WAG'}, {'month_id': 238, 'QAId': 'WAG'}, {'month_id': 239, 'QAId': 'WAG'}, {'month_id': 240, 'QAId': 'WAG'}, {'month_id': 241, 'QAId': 'WAG'}, {'month_id': 242, 'QAId': 'WAG'}, {'month_id': 243, 'QAId': 'WAG'}, {'month_id': 244, 'QAId': 'WAG'}, {'month_id': 245, 'QAId': 'WAG'}, {'month_id': 246, 'QAId': 'WAG'}, {'month_id': 247, 'QAId': 'WAG'}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "buy_list = []\n",
    "sell_list = []\n",
    "\n",
    "for j, stock in enumerate(chosen_stocks):\n",
    "  print(stock)\n",
    "  sorted_result = sorted(map(lambda x: x[j], result))\n",
    "  midpt = (sorted_result[-2] + sorted_result[1]) / 2\n",
    "  upper_threshold = midpt * 1.125\n",
    "  lower_threshold = midpt * 0.9875\n",
    "  \n",
    "  print(\"Target\\tPredict\\tConsequence\")\n",
    "  for i, r in enumerate(result):\n",
    "    prediction = r[j].item()\n",
    "    target = y_test[i][j].item()\n",
    "    buy_or_sell = 1 if prediction > upper_threshold else (-1 if prediction < lower_threshold else 0)\n",
    "    if prediction > upper_threshold:\n",
    "      buy_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    if prediction < lower_threshold:\n",
    "      sell_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    \n",
    "    to_print = str(target) + \"\\t\" + str(buy_or_sell)\n",
    "    if (buy_or_sell == -1 and target == 0) or (buy_or_sell == 1 and target == 1):\n",
    "      print(to_print + \"\\tGain\")\n",
    "    elif (buy_or_sell == -1 and target == 1) or (buy_or_sell == 1 and target == 0):\n",
    "      print(to_print + \"\\tLoss\")\n",
    "    else:\n",
    "      print(to_print + \"\\tNothing\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "print(buy_list)\n",
    "print(sell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1525762773847,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "tTp8yfv_ZSQv",
    "outputId": "0ee9cf79-1947-460b-d0cd-95a36f3d0f9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>QAId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [month_id, QAId]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBuy = pd.DataFrame(buy_list, columns=[\"month_id\", \"QAId\"]).sort_values([\"month_id\"])\n",
    "dfSell = pd.DataFrame(sell_list, columns=[\"month_id\", \"QAId\"]).sort_values([\"month_id\"])\n",
    "dfBuy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSell.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t2lSM7yu3dY7"
   },
   "outputs": [],
   "source": [
    "filename_base = \"_\".join([stock.lower() for stock in chosen_stocks])\n",
    "# filename_model = \"./\" + filename_base + \"_model.h5\"\n",
    "# filename_weights = \"./\" + filename_base + \"_weights.h5\"\n",
    "filename_output_buy = \"./\" + filename_base + \"_output_buy.csv\"\n",
    "filename_output_sell = \"./\" + filename_base + \"_output_sell.csv\"\n",
    "\n",
    "# model.save(filename_model)\n",
    "# model.save_weights(filename_weights)\n",
    "dfBuy.to_csv(filename_output_buy, index=False)\n",
    "dfSell.to_csv(filename_output_sell, index=False)\n",
    "\n",
    "# files.download(filename_model)\n",
    "# files.download(filename_weights)\n",
    "# files.download(filename_output_buy)\n",
    "# files.download(filename_output_sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "er00PkDpHjZ_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MSFT",
   "provenance": [
    {
     "file_id": "1DNgXa_HOyZehXtWnJ_rmfuch2xQ-gRCg",
     "timestamp": 1525762850051
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
