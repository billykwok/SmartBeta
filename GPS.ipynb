{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cE1kSExBZtrv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def to_month_id(year, month):\n",
    "  return (year - 1996) * 12 + month - 6\n",
    "\n",
    "n_features = 12 # or 54\n",
    "lookback = 3\n",
    "chosen_stocks = [\"GPS\"] # \"AMZN\", \"MSFT\", \"IBM\", \"INTC\", \"QCOM\", \"NVDA\", \"IBM\", \"ADBE\", \"EBAY\", \"CSCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 800,
     "status": "ok",
     "timestamp": 1525763876411,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "6vUqDR0MYQP1",
    "outputId": "ddc259d0-e2b6-4b12-e849-c602f12d1a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "dfFeature = pd.read_csv(\"./lstm_2004_12.csv\")\n",
    "# dfFeature.loc[dfFeature[\"return\"] == 0, \"return\"] = 1\n",
    "dfFeature = dfFeature[dfFeature.QAId.isin(chosen_stocks)]\n",
    "features = dfFeature.drop(columns=['month_id', 'QAId']).as_matrix()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1382,
     "status": "ok",
     "timestamp": 1525763878070,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "DwavdGT5wkIz",
    "outputId": "37dc165a-26f5-4bf4-c283-d635448bd707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalizedFeatures = MinMaxScaler().fit_transform(features) \\\n",
    "                                   .reshape(157, len(chosen_stocks), n_features) \\\n",
    "                                   .reshape(157, len(chosen_stocks) * n_features)\n",
    "print(normalizedFeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2118,
     "status": "ok",
     "timestamp": 1525763881161,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "ECiEkdPQ410b",
    "outputId": "7cada92e-95ac-400d-987d-cd9c6101549c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 1)\n"
     ]
    }
   ],
   "source": [
    "dfTarget = pd.read_csv(\"./return_2004_40.csv\")\n",
    "dfTarget[\"return\"] = np.sign(dfTarget[\"return\"])\n",
    "# dfTarget.loc[dfTarget[\"return\"] == 0, \"return\"] = 1\n",
    "dfTarget = dfTarget[dfTarget.QAId.isin(chosen_stocks)]\n",
    "dfTarget = dfTarget[dfTarget.month_id >= (to_month_id(2004, 1) + lookback)]\n",
    "targets = MinMaxScaler().fit_transform(dfTarget.drop(columns=['month_id', 'QAId']).as_matrix())\n",
    "y = targets.reshape(157 - lookback + 1, len(chosen_stocks))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1525763882966,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "Vo9QHxamwmbL",
    "outputId": "9ef70b36-2a3c-44be-a6ed-22930b63500a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billykwok/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(157, 12)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "splittedFeature = normalizedFeatures\n",
    "print(splittedFeature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2pHmaWhfZ4gm"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "x = np.zeros((157 - lookback + 1, lookback, n_features * len(chosen_stocks)))\n",
    "y_mock = np.zeros((157, len(chosen_stocks)))\n",
    "\n",
    "i = 0\n",
    "for train, test in TimeseriesGenerator(splittedFeature, y_mock, length=lookback, batch_size=1):\n",
    "  if i > 157 - lookback:\n",
    "    break\n",
    "  x[i] = train[0]\n",
    "  i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1525763885182,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "SkuyoR1bZ699",
    "outputId": "d82f1d27-ee9e-4198-f364-67e97179fdaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 3, 12)\n",
      "(81, 1)\n",
      "(36, 3, 12)\n",
      "(36, 1)\n",
      "(26, 3, 12)\n",
      "(26, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "test_months = 26\n",
    "end_point = 157 - lookback + 1\n",
    "split_point = 157 - lookback + 1 - test_months\n",
    "\n",
    "x_train = x[0:split_point - 48].reshape(split_point - 48, lookback, n_features * len(chosen_stocks))\n",
    "y_train = y[0:split_point - 48].reshape(split_point - 48, len(chosen_stocks))\n",
    "x_validate = x[split_point - 36:split_point].reshape(36, lookback, n_features * len(chosen_stocks))\n",
    "y_validate = y[split_point - 36:split_point].reshape(36, len(chosen_stocks))\n",
    "x_test = x[split_point:end_point].reshape(test_months, lookback, n_features * len(chosen_stocks))\n",
    "y_test = y[split_point:end_point].reshape(test_months, len(chosen_stocks))\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validate.shape)\n",
    "print(y_validate.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# ps = PredefinedSplit(np.append(np.negative(np.ones(60 * 465)), np.zeros(24 * 465))).split(x_train)\n",
    "\n",
    "# for train_ids, test_ids in ps:\n",
    "#   print(str(train_ids) + \", \" + str(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Co9Gsz7aZ_hb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Flatten, CuDNNLSTM\n",
    "from keras.regularizers import l1_l2, l2\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "\n",
    "np.random.seed(4103)\n",
    "\n",
    "def create_model(*param):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=256, input_shape=(lookback, n_features * len(chosen_stocks)), return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=128, return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LSTM(units=128, return_sequences=False))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(len(chosen_stocks), activation=\"relu\"))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001, decay=0.0), metrics=['accuracy'], *param)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13823,
     "status": "error",
     "timestamp": 1525764743477,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "0l18zd8xaQqR",
    "outputId": "47960b89-37a1-460d-aa9c-a58bbdff370f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81 samples, validate on 36 samples\n",
      "Epoch 1/200\n",
      "81/81 [==============================] - 3s 33ms/step - loss: 3.2270 - acc: 0.4691 - val_loss: 2.8055 - val_acc: 0.4167\n",
      "Epoch 2/200\n",
      "81/81 [==============================] - 0s 794us/step - loss: 2.5551 - acc: 0.4691 - val_loss: 2.5878 - val_acc: 0.4167\n",
      "Epoch 3/200\n",
      "81/81 [==============================] - 0s 784us/step - loss: 2.3720 - acc: 0.4691 - val_loss: 2.4398 - val_acc: 0.4167\n",
      "Epoch 4/200\n",
      "81/81 [==============================] - 0s 806us/step - loss: 2.2654 - acc: 0.4691 - val_loss: 2.3217 - val_acc: 0.4167\n",
      "Epoch 5/200\n",
      "81/81 [==============================] - 0s 849us/step - loss: 2.1435 - acc: 0.4691 - val_loss: 2.2224 - val_acc: 0.4167\n",
      "Epoch 6/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 2.0573 - acc: 0.4691 - val_loss: 2.1368 - val_acc: 0.4167\n",
      "Epoch 7/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 1.9790 - acc: 0.4691 - val_loss: 2.0620 - val_acc: 0.4167\n",
      "Epoch 8/200\n",
      "81/81 [==============================] - 0s 849us/step - loss: 1.9108 - acc: 0.4691 - val_loss: 1.9955 - val_acc: 0.4167\n",
      "Epoch 9/200\n",
      "81/81 [==============================] - 0s 833us/step - loss: 1.8465 - acc: 0.4691 - val_loss: 1.9359 - val_acc: 0.4167\n",
      "Epoch 10/200\n",
      "81/81 [==============================] - 0s 823us/step - loss: 1.7965 - acc: 0.4691 - val_loss: 1.8825 - val_acc: 0.4167\n",
      "Epoch 11/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 1.7539 - acc: 0.4691 - val_loss: 1.8324 - val_acc: 0.4167\n",
      "Epoch 12/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 1.7042 - acc: 0.4691 - val_loss: 1.7866 - val_acc: 0.4167\n",
      "Epoch 13/200\n",
      "81/81 [==============================] - 0s 792us/step - loss: 1.6592 - acc: 0.4691 - val_loss: 1.7446 - val_acc: 0.4167\n",
      "Epoch 14/200\n",
      "81/81 [==============================] - 0s 773us/step - loss: 1.6254 - acc: 0.4691 - val_loss: 1.7048 - val_acc: 0.4167\n",
      "Epoch 15/200\n",
      "81/81 [==============================] - 0s 762us/step - loss: 1.5976 - acc: 0.4691 - val_loss: 1.6670 - val_acc: 0.4167\n",
      "Epoch 16/200\n",
      "81/81 [==============================] - 0s 806us/step - loss: 1.5537 - acc: 0.4691 - val_loss: 1.6301 - val_acc: 0.4167\n",
      "Epoch 17/200\n",
      "81/81 [==============================] - 0s 775us/step - loss: 1.5223 - acc: 0.4691 - val_loss: 1.5953 - val_acc: 0.4167\n",
      "Epoch 18/200\n",
      "81/81 [==============================] - 0s 782us/step - loss: 1.4821 - acc: 0.4691 - val_loss: 1.5621 - val_acc: 0.4167\n",
      "Epoch 19/200\n",
      "81/81 [==============================] - 0s 730us/step - loss: 1.4617 - acc: 0.4691 - val_loss: 1.5298 - val_acc: 0.4167\n",
      "Epoch 20/200\n",
      "81/81 [==============================] - 0s 727us/step - loss: 1.4284 - acc: 0.4691 - val_loss: 1.4983 - val_acc: 0.4167\n",
      "Epoch 21/200\n",
      "81/81 [==============================] - 0s 830us/step - loss: 1.3997 - acc: 0.4691 - val_loss: 1.4676 - val_acc: 0.4167\n",
      "Epoch 22/200\n",
      "81/81 [==============================] - 0s 843us/step - loss: 1.3848 - acc: 0.4691 - val_loss: 1.4374 - val_acc: 0.4167\n",
      "Epoch 23/200\n",
      "81/81 [==============================] - 0s 833us/step - loss: 1.3436 - acc: 0.4691 - val_loss: 1.4079 - val_acc: 0.4167\n",
      "Epoch 24/200\n",
      "81/81 [==============================] - 0s 968us/step - loss: 1.3250 - acc: 0.4691 - val_loss: 1.3790 - val_acc: 0.4167\n",
      "Epoch 25/200\n",
      "81/81 [==============================] - 0s 948us/step - loss: 1.2937 - acc: 0.4691 - val_loss: 1.3506 - val_acc: 0.4167\n",
      "Epoch 26/200\n",
      "81/81 [==============================] - 0s 871us/step - loss: 1.2796 - acc: 0.4691 - val_loss: 1.3228 - val_acc: 0.4167\n",
      "Epoch 27/200\n",
      "81/81 [==============================] - 0s 816us/step - loss: 1.2437 - acc: 0.4691 - val_loss: 1.2952 - val_acc: 0.4167\n",
      "Epoch 28/200\n",
      "81/81 [==============================] - 0s 823us/step - loss: 1.2194 - acc: 0.4691 - val_loss: 1.2680 - val_acc: 0.4167\n",
      "Epoch 29/200\n",
      "81/81 [==============================] - 0s 825us/step - loss: 1.1999 - acc: 0.4691 - val_loss: 1.2410 - val_acc: 0.4167\n",
      "Epoch 30/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 1.1670 - acc: 0.4691 - val_loss: 1.2144 - val_acc: 0.4167\n",
      "Epoch 31/200\n",
      "81/81 [==============================] - 0s 829us/step - loss: 1.1471 - acc: 0.4691 - val_loss: 1.1888 - val_acc: 0.4167\n",
      "Epoch 32/200\n",
      "81/81 [==============================] - 0s 831us/step - loss: 1.1270 - acc: 0.4691 - val_loss: 1.1634 - val_acc: 0.4167\n",
      "Epoch 33/200\n",
      "81/81 [==============================] - 0s 856us/step - loss: 1.1025 - acc: 0.4691 - val_loss: 1.1384 - val_acc: 0.4167\n",
      "Epoch 34/200\n",
      "81/81 [==============================] - 0s 815us/step - loss: 1.0682 - acc: 0.4691 - val_loss: 1.1138 - val_acc: 0.4167\n",
      "Epoch 35/200\n",
      "81/81 [==============================] - 0s 823us/step - loss: 1.0560 - acc: 0.4691 - val_loss: 1.0902 - val_acc: 0.4167\n",
      "Epoch 36/200\n",
      "81/81 [==============================] - 0s 815us/step - loss: 1.0447 - acc: 0.4691 - val_loss: 1.0672 - val_acc: 0.4167\n",
      "Epoch 37/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 1.0149 - acc: 0.4691 - val_loss: 1.0445 - val_acc: 0.4167\n",
      "Epoch 38/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 0.9942 - acc: 0.4691 - val_loss: 1.0225 - val_acc: 0.4167\n",
      "Epoch 39/200\n",
      "81/81 [==============================] - 0s 818us/step - loss: 0.9801 - acc: 0.4691 - val_loss: 1.0005 - val_acc: 0.4167\n",
      "Epoch 40/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.9535 - acc: 0.4691 - val_loss: 0.9786 - val_acc: 0.4167\n",
      "Epoch 41/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 0.9438 - acc: 0.4691 - val_loss: 0.9578 - val_acc: 0.4167\n",
      "Epoch 42/200\n",
      "81/81 [==============================] - 0s 777us/step - loss: 0.9195 - acc: 0.4691 - val_loss: 0.9373 - val_acc: 0.4167\n",
      "Epoch 43/200\n",
      "81/81 [==============================] - 0s 786us/step - loss: 0.8931 - acc: 0.4691 - val_loss: 0.9170 - val_acc: 0.4167\n",
      "Epoch 44/200\n",
      "81/81 [==============================] - 0s 773us/step - loss: 0.8904 - acc: 0.4691 - val_loss: 0.8971 - val_acc: 0.4167\n",
      "Epoch 45/200\n",
      "81/81 [==============================] - 0s 790us/step - loss: 0.8724 - acc: 0.4691 - val_loss: 0.8781 - val_acc: 0.4167\n",
      "Epoch 46/200\n",
      "81/81 [==============================] - 0s 797us/step - loss: 0.8439 - acc: 0.4691 - val_loss: 0.8597 - val_acc: 0.4167\n",
      "Epoch 47/200\n",
      "81/81 [==============================] - 0s 776us/step - loss: 0.8312 - acc: 0.4691 - val_loss: 0.8422 - val_acc: 0.4167\n",
      "Epoch 48/200\n",
      "81/81 [==============================] - 0s 770us/step - loss: 0.8201 - acc: 0.4691 - val_loss: 0.8254 - val_acc: 0.4167\n",
      "Epoch 49/200\n",
      "81/81 [==============================] - 0s 777us/step - loss: 0.8153 - acc: 0.4691 - val_loss: 0.8092 - val_acc: 0.4167\n",
      "Epoch 50/200\n",
      "81/81 [==============================] - 0s 809us/step - loss: 0.7915 - acc: 0.4691 - val_loss: 0.7938 - val_acc: 0.4167\n",
      "Epoch 51/200\n",
      "81/81 [==============================] - 0s 797us/step - loss: 0.7812 - acc: 0.4691 - val_loss: 0.7797 - val_acc: 0.4167\n",
      "Epoch 52/200\n",
      "81/81 [==============================] - 0s 775us/step - loss: 0.7688 - acc: 0.4691 - val_loss: 0.7670 - val_acc: 0.4167\n",
      "Epoch 53/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 0.7573 - acc: 0.4691 - val_loss: 0.7550 - val_acc: 0.4167\n",
      "Epoch 54/200\n",
      "81/81 [==============================] - 0s 817us/step - loss: 0.7596 - acc: 0.4691 - val_loss: 0.7436 - val_acc: 0.4167\n",
      "Epoch 55/200\n",
      "81/81 [==============================] - 0s 806us/step - loss: 0.7466 - acc: 0.4691 - val_loss: 0.7335 - val_acc: 0.4167\n",
      "Epoch 56/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.7375 - acc: 0.4568 - val_loss: 0.7244 - val_acc: 0.4167\n",
      "Epoch 57/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.7407 - acc: 0.4691 - val_loss: 0.7167 - val_acc: 0.3611\n",
      "Epoch 58/200\n",
      "81/81 [==============================] - 0s 790us/step - loss: 0.7274 - acc: 0.4568 - val_loss: 0.7104 - val_acc: 0.3611\n",
      "Epoch 59/200\n",
      "81/81 [==============================] - 0s 851us/step - loss: 0.7199 - acc: 0.4568 - val_loss: 0.7044 - val_acc: 0.3889\n",
      "Epoch 60/200\n",
      "81/81 [==============================] - 0s 818us/step - loss: 0.7201 - acc: 0.3951 - val_loss: 0.6999 - val_acc: 0.4444\n",
      "Epoch 61/200\n",
      "81/81 [==============================] - 0s 793us/step - loss: 0.7247 - acc: 0.3951 - val_loss: 0.6965 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "81/81 [==============================] - 0s 848us/step - loss: 0.7142 - acc: 0.4198 - val_loss: 0.6938 - val_acc: 0.5833\n",
      "Epoch 63/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.7000 - acc: 0.5432 - val_loss: 0.6916 - val_acc: 0.6389\n",
      "Epoch 64/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.7208 - acc: 0.4568 - val_loss: 0.6901 - val_acc: 0.6389\n",
      "Epoch 65/200\n",
      "81/81 [==============================] - 0s 762us/step - loss: 0.7223 - acc: 0.3951 - val_loss: 0.6890 - val_acc: 0.6389\n",
      "Epoch 66/200\n",
      "81/81 [==============================] - 0s 774us/step - loss: 0.7166 - acc: 0.4815 - val_loss: 0.6883 - val_acc: 0.6389\n",
      "Epoch 67/200\n",
      "81/81 [==============================] - 0s 766us/step - loss: 0.7127 - acc: 0.4568 - val_loss: 0.6880 - val_acc: 0.6389\n",
      "Epoch 68/200\n",
      "81/81 [==============================] - 0s 868us/step - loss: 0.7166 - acc: 0.4568 - val_loss: 0.6881 - val_acc: 0.6389\n",
      "Epoch 69/200\n",
      "81/81 [==============================] - 0s 794us/step - loss: 0.7182 - acc: 0.4321 - val_loss: 0.6881 - val_acc: 0.6389\n",
      "Epoch 70/200\n",
      "81/81 [==============================] - 0s 819us/step - loss: 0.7195 - acc: 0.4444 - val_loss: 0.6877 - val_acc: 0.5833\n",
      "Epoch 71/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.7136 - acc: 0.4691 - val_loss: 0.6876 - val_acc: 0.5833\n",
      "Epoch 72/200\n",
      "81/81 [==============================] - 0s 811us/step - loss: 0.7132 - acc: 0.4815 - val_loss: 0.6875 - val_acc: 0.5833\n",
      "Epoch 73/200\n",
      "81/81 [==============================] - 0s 812us/step - loss: 0.7024 - acc: 0.4815 - val_loss: 0.6873 - val_acc: 0.5833\n",
      "Epoch 74/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.7077 - acc: 0.4568 - val_loss: 0.6871 - val_acc: 0.5833\n",
      "Epoch 75/200\n",
      "81/81 [==============================] - 0s 835us/step - loss: 0.7244 - acc: 0.4691 - val_loss: 0.6870 - val_acc: 0.5833\n",
      "Epoch 76/200\n",
      "81/81 [==============================] - 0s 816us/step - loss: 0.7171 - acc: 0.4815 - val_loss: 0.6872 - val_acc: 0.5833\n",
      "Epoch 77/200\n",
      "81/81 [==============================] - 0s 783us/step - loss: 0.7024 - acc: 0.5062 - val_loss: 0.6873 - val_acc: 0.5833\n",
      "Epoch 78/200\n",
      "81/81 [==============================] - 0s 787us/step - loss: 0.7050 - acc: 0.5185 - val_loss: 0.6872 - val_acc: 0.5833\n",
      "Epoch 79/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.7055 - acc: 0.4938 - val_loss: 0.6872 - val_acc: 0.5833\n",
      "Epoch 80/200\n",
      "81/81 [==============================] - 0s 824us/step - loss: 0.6950 - acc: 0.5185 - val_loss: 0.6873 - val_acc: 0.5833\n",
      "Epoch 81/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.7186 - acc: 0.4691 - val_loss: 0.6874 - val_acc: 0.5833\n",
      "Epoch 82/200\n",
      "81/81 [==============================] - 0s 751us/step - loss: 0.7192 - acc: 0.4444 - val_loss: 0.6876 - val_acc: 0.5833\n",
      "Epoch 83/200\n",
      "81/81 [==============================] - 0s 767us/step - loss: 0.7134 - acc: 0.4444 - val_loss: 0.6876 - val_acc: 0.5833\n",
      "Epoch 84/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.6964 - acc: 0.5432 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 85/200\n",
      "81/81 [==============================] - 0s 748us/step - loss: 0.7130 - acc: 0.4568 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 86/200\n",
      "81/81 [==============================] - 0s 763us/step - loss: 0.7127 - acc: 0.4321 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 87/200\n",
      "81/81 [==============================] - 0s 827us/step - loss: 0.7032 - acc: 0.4938 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 88/200\n",
      "81/81 [==============================] - 0s 782us/step - loss: 0.7118 - acc: 0.5309 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 89/200\n",
      "81/81 [==============================] - 0s 819us/step - loss: 0.7066 - acc: 0.4815 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 90/200\n",
      "81/81 [==============================] - 0s 821us/step - loss: 0.7106 - acc: 0.4198 - val_loss: 0.6886 - val_acc: 0.5833\n",
      "Epoch 91/200\n",
      "81/81 [==============================] - 0s 817us/step - loss: 0.7216 - acc: 0.3951 - val_loss: 0.6887 - val_acc: 0.5833\n",
      "Epoch 92/200\n",
      "81/81 [==============================] - 0s 793us/step - loss: 0.7134 - acc: 0.4691 - val_loss: 0.6887 - val_acc: 0.5833\n",
      "Epoch 93/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.7097 - acc: 0.4815 - val_loss: 0.6887 - val_acc: 0.5833\n",
      "Epoch 94/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.7023 - acc: 0.4444 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 95/200\n",
      "81/81 [==============================] - 0s 810us/step - loss: 0.7046 - acc: 0.4568 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 96/200\n",
      "81/81 [==============================] - 0s 843us/step - loss: 0.7052 - acc: 0.4568 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 97/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.6884 - acc: 0.5185 - val_loss: 0.6886 - val_acc: 0.5833\n",
      "Epoch 98/200\n",
      "81/81 [==============================] - 0s 815us/step - loss: 0.7037 - acc: 0.4444 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 99/200\n",
      "81/81 [==============================] - 0s 769us/step - loss: 0.7069 - acc: 0.5309 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 100/200\n",
      "81/81 [==============================] - 0s 768us/step - loss: 0.7082 - acc: 0.4815 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 101/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 0.7101 - acc: 0.4691 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 102/200\n",
      "81/81 [==============================] - 0s 788us/step - loss: 0.6968 - acc: 0.5309 - val_loss: 0.6879 - val_acc: 0.5833\n",
      "Epoch 103/200\n",
      "81/81 [==============================] - 0s 831us/step - loss: 0.7081 - acc: 0.4815 - val_loss: 0.6876 - val_acc: 0.5833\n",
      "Epoch 104/200\n",
      "81/81 [==============================] - 0s 783us/step - loss: 0.6986 - acc: 0.5309 - val_loss: 0.6875 - val_acc: 0.5833\n",
      "Epoch 105/200\n",
      "81/81 [==============================] - 0s 836us/step - loss: 0.6997 - acc: 0.5309 - val_loss: 0.6873 - val_acc: 0.5833\n",
      "Epoch 106/200\n",
      "81/81 [==============================] - 0s 808us/step - loss: 0.7002 - acc: 0.5062 - val_loss: 0.6871 - val_acc: 0.5833\n",
      "Epoch 107/200\n",
      "81/81 [==============================] - 0s 780us/step - loss: 0.6976 - acc: 0.5062 - val_loss: 0.6869 - val_acc: 0.5833\n",
      "Epoch 108/200\n",
      "81/81 [==============================] - 0s 763us/step - loss: 0.7221 - acc: 0.3580 - val_loss: 0.6868 - val_acc: 0.5833\n",
      "Epoch 109/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 0.6953 - acc: 0.5432 - val_loss: 0.6867 - val_acc: 0.5833\n",
      "Epoch 110/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 0.7177 - acc: 0.4568 - val_loss: 0.6867 - val_acc: 0.5833\n",
      "Epoch 111/200\n",
      "81/81 [==============================] - 0s 830us/step - loss: 0.7172 - acc: 0.4444 - val_loss: 0.6873 - val_acc: 0.5833\n",
      "Epoch 112/200\n",
      "81/81 [==============================] - 0s 823us/step - loss: 0.6978 - acc: 0.5556 - val_loss: 0.6879 - val_acc: 0.5833\n",
      "Epoch 113/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.7055 - acc: 0.5556 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 114/200\n",
      "81/81 [==============================] - 0s 849us/step - loss: 0.6968 - acc: 0.4815 - val_loss: 0.6889 - val_acc: 0.5833\n",
      "Epoch 115/200\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 0.7040 - acc: 0.5432 - val_loss: 0.6894 - val_acc: 0.5833\n",
      "Epoch 116/200\n",
      "81/81 [==============================] - 0s 892us/step - loss: 0.7059 - acc: 0.5185 - val_loss: 0.6897 - val_acc: 0.5556\n",
      "Epoch 117/200\n",
      "81/81 [==============================] - 0s 851us/step - loss: 0.7182 - acc: 0.4321 - val_loss: 0.6899 - val_acc: 0.5556\n",
      "Epoch 118/200\n",
      "81/81 [==============================] - 0s 788us/step - loss: 0.7018 - acc: 0.5185 - val_loss: 0.6902 - val_acc: 0.5833\n",
      "Epoch 119/200\n",
      "81/81 [==============================] - 0s 790us/step - loss: 0.7024 - acc: 0.4568 - val_loss: 0.6902 - val_acc: 0.5556\n",
      "Epoch 120/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 0.6996 - acc: 0.5432 - val_loss: 0.6902 - val_acc: 0.5556\n",
      "Epoch 121/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 0.6981 - acc: 0.5432 - val_loss: 0.6902 - val_acc: 0.5556\n",
      "Epoch 122/200\n",
      "81/81 [==============================] - 0s 787us/step - loss: 0.6987 - acc: 0.5556 - val_loss: 0.6901 - val_acc: 0.5556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "81/81 [==============================] - 0s 742us/step - loss: 0.6920 - acc: 0.5556 - val_loss: 0.6900 - val_acc: 0.5556\n",
      "Epoch 124/200\n",
      "81/81 [==============================] - 0s 782us/step - loss: 0.7033 - acc: 0.4568 - val_loss: 0.6902 - val_acc: 0.5556\n",
      "Epoch 125/200\n",
      "81/81 [==============================] - 0s 757us/step - loss: 0.6978 - acc: 0.5556 - val_loss: 0.6900 - val_acc: 0.5556\n",
      "Epoch 126/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 0.6929 - acc: 0.5309 - val_loss: 0.6897 - val_acc: 0.5556\n",
      "Epoch 127/200\n",
      "81/81 [==============================] - 0s 771us/step - loss: 0.7075 - acc: 0.5185 - val_loss: 0.6891 - val_acc: 0.5833\n",
      "Epoch 128/200\n",
      "81/81 [==============================] - 0s 770us/step - loss: 0.7049 - acc: 0.4568 - val_loss: 0.6888 - val_acc: 0.5833\n",
      "Epoch 129/200\n",
      "81/81 [==============================] - 0s 774us/step - loss: 0.6988 - acc: 0.5062 - val_loss: 0.6886 - val_acc: 0.5833\n",
      "Epoch 130/200\n",
      "81/81 [==============================] - 0s 783us/step - loss: 0.7014 - acc: 0.5556 - val_loss: 0.6885 - val_acc: 0.5833\n",
      "Epoch 131/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 0.7036 - acc: 0.5062 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 132/200\n",
      "81/81 [==============================] - 0s 778us/step - loss: 0.6725 - acc: 0.6049 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 133/200\n",
      "81/81 [==============================] - 0s 809us/step - loss: 0.7100 - acc: 0.4568 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 134/200\n",
      "81/81 [==============================] - 0s 776us/step - loss: 0.6856 - acc: 0.5556 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 135/200\n",
      "81/81 [==============================] - 0s 795us/step - loss: 0.6871 - acc: 0.5802 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 136/200\n",
      "81/81 [==============================] - 0s 785us/step - loss: 0.6787 - acc: 0.5926 - val_loss: 0.6877 - val_acc: 0.5833\n",
      "Epoch 137/200\n",
      "81/81 [==============================] - 0s 776us/step - loss: 0.6956 - acc: 0.5309 - val_loss: 0.6878 - val_acc: 0.5833\n",
      "Epoch 138/200\n",
      "81/81 [==============================] - 0s 785us/step - loss: 0.6987 - acc: 0.5679 - val_loss: 0.6879 - val_acc: 0.5833\n",
      "Epoch 139/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.7035 - acc: 0.4938 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 140/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.7008 - acc: 0.5062 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 141/200\n",
      "81/81 [==============================] - 0s 827us/step - loss: 0.6974 - acc: 0.5062 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 142/200\n",
      "81/81 [==============================] - 0s 789us/step - loss: 0.7005 - acc: 0.5432 - val_loss: 0.6888 - val_acc: 0.5833\n",
      "Epoch 143/200\n",
      "81/81 [==============================] - 0s 777us/step - loss: 0.7016 - acc: 0.4938 - val_loss: 0.6893 - val_acc: 0.5833\n",
      "Epoch 144/200\n",
      "81/81 [==============================] - 0s 760us/step - loss: 0.6937 - acc: 0.5185 - val_loss: 0.6893 - val_acc: 0.5833\n",
      "Epoch 145/200\n",
      "81/81 [==============================] - 0s 768us/step - loss: 0.6914 - acc: 0.5309 - val_loss: 0.6896 - val_acc: 0.5833\n",
      "Epoch 146/200\n",
      "81/81 [==============================] - 0s 723us/step - loss: 0.6830 - acc: 0.5556 - val_loss: 0.6895 - val_acc: 0.5833\n",
      "Epoch 147/200\n",
      "81/81 [==============================] - 0s 731us/step - loss: 0.6884 - acc: 0.5432 - val_loss: 0.6894 - val_acc: 0.5833\n",
      "Epoch 148/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.6955 - acc: 0.4815 - val_loss: 0.6890 - val_acc: 0.5833\n",
      "Epoch 149/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.6910 - acc: 0.5679 - val_loss: 0.6886 - val_acc: 0.5833\n",
      "Epoch 150/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.6972 - acc: 0.6049 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 151/200\n",
      "81/81 [==============================] - 0s 826us/step - loss: 0.6913 - acc: 0.5802 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 152/200\n",
      "81/81 [==============================] - 0s 783us/step - loss: 0.6848 - acc: 0.5556 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 153/200\n",
      "81/81 [==============================] - 0s 779us/step - loss: 0.6852 - acc: 0.5185 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 154/200\n",
      "81/81 [==============================] - 0s 800us/step - loss: 0.6954 - acc: 0.5679 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 155/200\n",
      "81/81 [==============================] - 0s 844us/step - loss: 0.6918 - acc: 0.5926 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 156/200\n",
      "81/81 [==============================] - 0s 788us/step - loss: 0.6865 - acc: 0.5185 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 157/200\n",
      "81/81 [==============================] - 0s 809us/step - loss: 0.7010 - acc: 0.4815 - val_loss: 0.6879 - val_acc: 0.5833\n",
      "Epoch 158/200\n",
      "81/81 [==============================] - 0s 807us/step - loss: 0.6789 - acc: 0.5556 - val_loss: 0.6881 - val_acc: 0.5833\n",
      "Epoch 159/200\n",
      "81/81 [==============================] - 0s 818us/step - loss: 0.6846 - acc: 0.5802 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 160/200\n",
      "81/81 [==============================] - 0s 815us/step - loss: 0.6888 - acc: 0.5185 - val_loss: 0.6882 - val_acc: 0.5833\n",
      "Epoch 161/200\n",
      "81/81 [==============================] - 0s 773us/step - loss: 0.6985 - acc: 0.5062 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 162/200\n",
      "81/81 [==============================] - 0s 769us/step - loss: 0.6820 - acc: 0.6049 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 163/200\n",
      "81/81 [==============================] - 0s 819us/step - loss: 0.6836 - acc: 0.5802 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 164/200\n",
      "81/81 [==============================] - 0s 838us/step - loss: 0.7042 - acc: 0.4691 - val_loss: 0.6883 - val_acc: 0.5833\n",
      "Epoch 165/200\n",
      "81/81 [==============================] - 0s 832us/step - loss: 0.6944 - acc: 0.5556 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 166/200\n",
      "81/81 [==============================] - 0s 792us/step - loss: 0.6832 - acc: 0.5309 - val_loss: 0.6884 - val_acc: 0.5833\n",
      "Epoch 167/200\n",
      "81/81 [==============================] - 0s 815us/step - loss: 0.6843 - acc: 0.5926 - val_loss: 0.6887 - val_acc: 0.5833\n",
      "Epoch 168/200\n",
      "81/81 [==============================] - 0s 798us/step - loss: 0.6909 - acc: 0.5432 - val_loss: 0.6888 - val_acc: 0.5833\n",
      "Epoch 169/200\n",
      "81/81 [==============================] - 0s 781us/step - loss: 0.6963 - acc: 0.5679 - val_loss: 0.6891 - val_acc: 0.5833\n",
      "Epoch 170/200\n",
      "81/81 [==============================] - 0s 802us/step - loss: 0.6946 - acc: 0.5556 - val_loss: 0.6889 - val_acc: 0.5833\n",
      "Epoch 171/200\n",
      "81/81 [==============================] - 0s 817us/step - loss: 0.6867 - acc: 0.5556 - val_loss: 0.6888 - val_acc: 0.5833\n",
      "Epoch 172/200\n",
      "81/81 [==============================] - 0s 822us/step - loss: 0.6931 - acc: 0.5679 - val_loss: 0.6888 - val_acc: 0.5833\n",
      "Epoch 173/200\n",
      "81/81 [==============================] - 0s 810us/step - loss: 0.6867 - acc: 0.5679 - val_loss: 0.6887 - val_acc: 0.5833\n",
      "Epoch 174/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.6938 - acc: 0.5309 - val_loss: 0.6891 - val_acc: 0.5833\n",
      "Epoch 175/200\n",
      "81/81 [==============================] - 0s 833us/step - loss: 0.7019 - acc: 0.5432 - val_loss: 0.6894 - val_acc: 0.5833\n",
      "Epoch 176/200\n",
      "81/81 [==============================] - 0s 868us/step - loss: 0.6851 - acc: 0.5679 - val_loss: 0.6900 - val_acc: 0.6111\n",
      "Epoch 177/200\n",
      "81/81 [==============================] - 0s 799us/step - loss: 0.6981 - acc: 0.5432 - val_loss: 0.6906 - val_acc: 0.5833\n",
      "Epoch 178/200\n",
      "81/81 [==============================] - 0s 829us/step - loss: 0.7001 - acc: 0.5556 - val_loss: 0.6909 - val_acc: 0.5278\n",
      "Epoch 179/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.6843 - acc: 0.5432 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 180/200\n",
      "81/81 [==============================] - 0s 828us/step - loss: 0.6885 - acc: 0.5802 - val_loss: 0.6913 - val_acc: 0.5278\n",
      "Epoch 181/200\n",
      "81/81 [==============================] - 0s 813us/step - loss: 0.6902 - acc: 0.5432 - val_loss: 0.6911 - val_acc: 0.5278\n",
      "Epoch 182/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 0.6832 - acc: 0.5679 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 183/200\n",
      "81/81 [==============================] - 0s 803us/step - loss: 0.6910 - acc: 0.5185 - val_loss: 0.6909 - val_acc: 0.5278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200\n",
      "81/81 [==============================] - 0s 801us/step - loss: 0.6873 - acc: 0.5679 - val_loss: 0.6903 - val_acc: 0.6111\n",
      "Epoch 185/200\n",
      "81/81 [==============================] - 0s 772us/step - loss: 0.6851 - acc: 0.5802 - val_loss: 0.6901 - val_acc: 0.6111\n",
      "Epoch 186/200\n",
      "81/81 [==============================] - 0s 766us/step - loss: 0.6831 - acc: 0.6049 - val_loss: 0.6899 - val_acc: 0.6111\n",
      "Epoch 187/200\n",
      "81/81 [==============================] - 0s 831us/step - loss: 0.6923 - acc: 0.5309 - val_loss: 0.6901 - val_acc: 0.6111\n",
      "Epoch 188/200\n",
      "81/81 [==============================] - 0s 814us/step - loss: 0.6897 - acc: 0.5679 - val_loss: 0.6904 - val_acc: 0.6111\n",
      "Epoch 189/200\n",
      "81/81 [==============================] - 0s 788us/step - loss: 0.6930 - acc: 0.5432 - val_loss: 0.6906 - val_acc: 0.5556\n",
      "Epoch 190/200\n",
      "81/81 [==============================] - 0s 880us/step - loss: 0.6896 - acc: 0.5802 - val_loss: 0.6907 - val_acc: 0.5278\n",
      "Epoch 191/200\n",
      "81/81 [==============================] - 0s 828us/step - loss: 0.7016 - acc: 0.5679 - val_loss: 0.6906 - val_acc: 0.5556\n",
      "Epoch 192/200\n",
      "81/81 [==============================] - 0s 894us/step - loss: 0.6821 - acc: 0.6049 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 193/200\n",
      "81/81 [==============================] - 0s 841us/step - loss: 0.6799 - acc: 0.5802 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 194/200\n",
      "81/81 [==============================] - 0s 806us/step - loss: 0.6909 - acc: 0.5185 - val_loss: 0.6910 - val_acc: 0.5278\n",
      "Epoch 195/200\n",
      "81/81 [==============================] - 0s 817us/step - loss: 0.6964 - acc: 0.5309 - val_loss: 0.6914 - val_acc: 0.5278\n",
      "Epoch 196/200\n",
      "81/81 [==============================] - 0s 805us/step - loss: 0.6804 - acc: 0.5802 - val_loss: 0.6917 - val_acc: 0.5278\n",
      "Epoch 197/200\n",
      "81/81 [==============================] - 0s 800us/step - loss: 0.6940 - acc: 0.5432 - val_loss: 0.6919 - val_acc: 0.5278\n",
      "Epoch 198/200\n",
      "81/81 [==============================] - 0s 784us/step - loss: 0.7007 - acc: 0.4815 - val_loss: 0.6921 - val_acc: 0.5000\n",
      "Epoch 199/200\n",
      "81/81 [==============================] - 0s 791us/step - loss: 0.6833 - acc: 0.5556 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 200/200\n",
      "81/81 [==============================] - 0s 804us/step - loss: 0.6906 - acc: 0.5679 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "<keras.callbacks.History object at 0x1a2e07e9e8>\n"
     ]
    }
   ],
   "source": [
    "batch_size=30\n",
    "\n",
    "model = create_model()\n",
    "train_score = model.fit(x_train, y_train, validation_data=(x_validate, y_validate), batch_size=batch_size, epochs=200)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2955
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1157,
     "status": "error",
     "timestamp": 1525763095237,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "yycG9fQgaTWR",
    "outputId": "6bc04992-aa67-445c-a5dc-4034a301660a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 298us/step\n",
      "loss: 0.7243540287017822\n",
      "acc: 0.38461539149284363\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2e0afba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_score = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "for i, mn in enumerate(model.metrics_names):\n",
    "  print(mn + \": \" + str(test_score[i]))\n",
    "\n",
    "plt.plot(train_score.history[\"loss\"])\n",
    "plt.plot(train_score.history[\"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Metrics\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fYwRfk9VfdKR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, classification_report\n",
    "\n",
    "def threshold_tester(result, target, a, b):\n",
    "  for t in np.arange(a, b + 0.05, 0.05):\n",
    "    print(\"thresholder: \" + str(t))\n",
    "    print(classification_report(target, (result >= t).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2921
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 892,
     "status": "error",
     "timestamp": 1525762991314,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "zQ-ZbuKz3MVJ",
    "outputId": "12722ed0-2978-4350-8022-880def161a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6220798 ]\n",
      " [0.6196227 ]\n",
      " [0.591205  ]\n",
      " [0.598688  ]\n",
      " [0.5629836 ]\n",
      " [0.5437742 ]\n",
      " [0.56386024]\n",
      " [0.54422605]\n",
      " [0.53673214]\n",
      " [0.5395769 ]\n",
      " [0.5588185 ]\n",
      " [0.60258454]\n",
      " [0.59688514]\n",
      " [0.5866992 ]\n",
      " [0.5957302 ]\n",
      " [0.57812524]\n",
      " [0.53265095]\n",
      " [0.50890577]\n",
      " [0.56756735]\n",
      " [0.62948287]\n",
      " [0.5723905 ]\n",
      " [0.5216316 ]\n",
      " [0.5271014 ]\n",
      " [0.554122  ]\n",
      " [0.512435  ]\n",
      " [0.5116066 ]]\n",
      "thresholder: 0.1\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.15000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.20000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.25000000000000006\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.30000000000000004\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.3500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.40000000000000013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.45000000000000007\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.5000000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00        16\n",
      "        1.0       0.38      1.00      0.56        10\n",
      "\n",
      "avg / total       0.15      0.38      0.21        26\n",
      "\n",
      "thresholder: 0.5500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.44      0.54        16\n",
      "        1.0       0.44      0.70      0.54        10\n",
      "\n",
      "avg / total       0.60      0.54      0.54        26\n",
      "\n",
      "thresholder: 0.6000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.94      0.79        16\n",
      "        1.0       0.75      0.30      0.43        10\n",
      "\n",
      "avg / total       0.71      0.69      0.65        26\n",
      "\n",
      "thresholder: 0.6500000000000001\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n",
      "thresholder: 0.7000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n",
      "thresholder: 0.7500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n",
      "thresholder: 0.8000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n",
      "thresholder: 0.8500000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n",
      "thresholder: 0.9000000000000002\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.76        16\n",
      "        1.0       0.00      0.00      0.00        10\n",
      "\n",
      "avg / total       0.38      0.62      0.47        26\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billykwok/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(x_test, batch_size=batch_size)\n",
    "print(result)\n",
    "threshold_tester(result, y_test, 0.1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1525762727684,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "qcA2GSqdINA-",
    "outputId": "b09b9962-ff00-4a8f-c628-e0e0f9877b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPS\n",
      "Target\tPredict\tConsequence\n",
      "1.0\t1\tGain\n",
      "1.0\t1\tGain\n",
      "0.0\t0\tNothing\n",
      "0.0\t1\tLoss\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t0\tNothing\n",
      "0.0\t0\tNothing\n",
      "1.0\t1\tGain\n",
      "1.0\t1\tGain\n",
      "1.0\t0\tNothing\n",
      "0.0\t1\tLoss\n",
      "0.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "1.0\t-1\tLoss\n",
      "0.0\t0\tNothing\n",
      "0.0\t1\tLoss\n",
      "1.0\t0\tNothing\n",
      "0.0\t-1\tGain\n",
      "0.0\t-1\tGain\n",
      "1.0\t0\tNothing\n",
      "1.0\t-1\tLoss\n",
      "0.0\t-1\tGain\n",
      "\n",
      "\n",
      "[{'month_id': 223, 'QAId': 'GPS'}, {'month_id': 224, 'QAId': 'GPS'}, {'month_id': 226, 'QAId': 'GPS'}, {'month_id': 234, 'QAId': 'GPS'}, {'month_id': 235, 'QAId': 'GPS'}, {'month_id': 237, 'QAId': 'GPS'}, {'month_id': 242, 'QAId': 'GPS'}]\n",
      "[{'month_id': 231, 'QAId': 'GPS'}, {'month_id': 239, 'QAId': 'GPS'}, {'month_id': 240, 'QAId': 'GPS'}, {'month_id': 244, 'QAId': 'GPS'}, {'month_id': 245, 'QAId': 'GPS'}, {'month_id': 247, 'QAId': 'GPS'}, {'month_id': 248, 'QAId': 'GPS'}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "buy_list = []\n",
    "sell_list = []\n",
    "\n",
    "for j, stock in enumerate(chosen_stocks):\n",
    "  print(stock)\n",
    "  sorted_result = sorted(map(lambda x: x[j], result))\n",
    "  midpt = (sorted_result[-2] + sorted_result[1]) / 2\n",
    "  upper_threshold = midpt * 1.05\n",
    "  lower_threshold = midpt * 0.95\n",
    "  \n",
    "  print(\"Target\\tPredict\\tConsequence\")\n",
    "  for i, r in enumerate(result):\n",
    "    prediction = r[j].item()\n",
    "    target = y_test[i][j].item()\n",
    "    buy_or_sell = 1 if prediction > upper_threshold else (-1 if prediction < lower_threshold else 0)\n",
    "    if prediction > upper_threshold:\n",
    "      buy_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    if prediction < lower_threshold:\n",
    "      sell_list.append({'month_id': i + 223, 'QAId': stock})\n",
    "    \n",
    "    to_print = str(target) + \"\\t\" + str(buy_or_sell)\n",
    "    if (buy_or_sell == -1 and target == 0) or (buy_or_sell == 1 and target == 1):\n",
    "      print(to_print + \"\\tGain\")\n",
    "    elif (buy_or_sell == -1 and target == 1) or (buy_or_sell == 1 and target == 0):\n",
    "      print(to_print + \"\\tLoss\")\n",
    "    else:\n",
    "      print(to_print + \"\\tNothing\")\n",
    "  print(\"\\n\")\n",
    "\n",
    "print(buy_list)\n",
    "print(sell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1525762773847,
     "user": {
      "displayName": "Billy Kwok",
      "photoUrl": "//lh6.googleusercontent.com/-hjLXUpOC6dQ/AAAAAAAAAAI/AAAAAAAACok/e_QRGsCrNhg/s50-c-k-no/photo.jpg",
      "userId": "105012465118603790454"
     },
     "user_tz": -480
    },
    "id": "tTp8yfv_ZSQv",
    "outputId": "0ee9cf79-1947-460b-d0cd-95a36f3d0f9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>QAId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>223</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>235</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id QAId\n",
       "0       223  GPS\n",
       "1       224  GPS\n",
       "2       226  GPS\n",
       "3       234  GPS\n",
       "4       235  GPS"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfBuy = pd.DataFrame(buy_list, columns=[\"month_id\", \"QAId\"])\n",
    "dfSell = pd.DataFrame(sell_list, columns=[\"month_id\", \"QAId\"])\n",
    "dfBuy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month_id</th>\n",
       "      <th>QAId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>239</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>245</td>\n",
       "      <td>GPS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month_id QAId\n",
       "0       231  GPS\n",
       "1       239  GPS\n",
       "2       240  GPS\n",
       "3       244  GPS\n",
       "4       245  GPS"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSell.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "t2lSM7yu3dY7"
   },
   "outputs": [],
   "source": [
    "filename_base = \"_\".join([stock.lower() for stock in chosen_stocks])\n",
    "# filename_model = \"./\" + filename_base + \"_model.h5\"\n",
    "# filename_weights = \"./\" + filename_base + \"_weights.h5\"\n",
    "filename_output_buy = \"./\" + filename_base + \"_output_buy.h5\"\n",
    "filename_output_sell = \"./\" + filename_base + \"_output_sell.h5\"\n",
    "\n",
    "# model.save(filename_model)\n",
    "# model.save_weights(filename_weights)\n",
    "dfBuy.to_csv(filename_output_buy, index=False)\n",
    "dfSell.to_csv(filename_output_sell, index=False)\n",
    "\n",
    "# files.download(filename_model)\n",
    "# files.download(filename_weights)\n",
    "# files.download(filename_output_buy)\n",
    "# files.download(filename_output_sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "er00PkDpHjZ_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "MSFT",
   "provenance": [
    {
     "file_id": "1DNgXa_HOyZehXtWnJ_rmfuch2xQ-gRCg",
     "timestamp": 1525762850051
    }
   ],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
